{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "import sys\n",
    "sys.path.insert(0, os.getcwd()+\"/../../../../libs/common\")\n",
    "from FileUtil import gpfs2WinPath\n",
    "\n",
    "caldatafile = r'/gpfs/WW/BD/MXP/SHARED/SEM_IMAGE/IMEC/Case02_calaveras_v3/3Tmp/CT_KPI_test/Calaveras_v3_regular_CT_KPI_003_slope_modified_revert_all_patterns/h/cache/dummydb/result/MXP/job1/ContourSelectModelCalibration460result1/caldata.txt'\n",
    "caldatafile = gpfs2WinPath(caldatafile)\n",
    "\n",
    "df = pd.read_csv(caldatafile, sep='\\s+')\n",
    "\n",
    "tgtColName = 'UserLabel'\n",
    "neighborColNames = ['NeighborOrientation', 'NeighborParalism']#, 'NeighborParalism', 'NeighborContinuity',\n",
    "allColNames = ['NeighborContinuity', 'NeighborOrientation', 'NeighborParalism', 'slope', 'intensity', 'ridge_intensity', 'contrast', 'EigenRatio']\n",
    "srcColNames = ['slope', 'intensity', 'ridge_intensity', 'NeighborOrientation', 'NeighborParalism']\n",
    "tgtColName = 'UserLabel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, tgtColName].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, srcColNames].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiScaling = 1\n",
    "scaling = lambda X_Arr: np.array([(X_Arr[i] - Xmin[i])/(Xmax[i] - Xmin[i]) for i in range(len(srcColNames)) ])\n",
    "if wiScaling:\n",
    "    X_cal = df.loc[df.usage=='CAL', srcColNames].values\n",
    "    Xmin = X_cal.min(axis=0)\n",
    "    Xmax = X_cal.max(axis=0)\n",
    "    Xminmax = pd.DataFrame(data= np.array([Xmin, Xmax]), columns=srcColNames, index=['min', 'max'])\n",
    "    print(Xminmax)\n",
    "    df.loc[:, srcColNames] = df.loc[:, srcColNames].apply(scaling, axis=1)\n",
    "\n",
    "X_cal = df.loc[df.usage=='CAL', srcColNames].values\n",
    "y_cal = df.loc[df.usage=='CAL', tgtColName].values\n",
    "X_ver = df.loc[df.usage=='VER', srcColNames].values\n",
    "y_ver = df.loc[df.usage=='VER', tgtColName].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, srcColNames].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "\n",
    "# hist plot \n",
    "\n",
    "df2 = pd.melt(df, id_vars=tgtColName, value_vars=neighborColNames, value_name='value')\n",
    "bins=np.linspace(df2.value.min(), df2.value.max(), 100)\n",
    "g = sns.FacetGrid(df2, col=\"variable\", hue=tgtColName, palette=\"Set1\", col_wrap=3)\n",
    "g.map(plt.hist, 'value', bins=bins, ec=\"k\")\n",
    "plt.yscale('log')\n",
    "g.axes[-1].legend()\n",
    "\n",
    "# scatter plot \n",
    "#sns.pointplot('NeighborOrientation', 'NeighborParalism', hue=tgtColName, data=df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "def sephist(col):\n",
    "    TP = df[df[tgtColName] == 0][col]\n",
    "    TN = df[df[tgtColName] == 1][col]\n",
    "    return TP, TN\n",
    "#df.loc[:, 'slope'] = df.loc[:, 'slope'].abs()\n",
    "for num, alpha in enumerate(allColNames):\n",
    "    plt.subplot(2, 4, num+1)\n",
    "\n",
    "    TP, TN = sephist(alpha)\n",
    "    plt.hist((TP, TN), bins=25, alpha=0.5, label=map(''.join, zip(2*[tgtColName], 2*['=='], ['0', '1'])), color=['b', 'g'])\n",
    "    #plt.hist(TP, bins=50, alpha=0.5, label=tgtColName+'==0', color='b')\n",
    "    #plt.hist(TN, bins=50, alpha=0.5, label=tgtColName+'==1', color='g')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(alpha)\n",
    "    plt.yscale('log')\n",
    "#plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, tgtColName].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[df[tgtColName]==0, srcColNames].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[df[tgtColName]==1, srcColNames].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', class_weight='balanced') # {0: 10, 1: 1}\n",
    "model = clf.fit(X_cal, y_cal)\n",
    "\n",
    "modelform = pd.DataFrame(data=clf.coef_.flatten(), index=srcColNames)\n",
    "modelform.loc['intercept', 0] = clf.intercept_\n",
    "print(modelform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(random_state=0) # , max_depth=len(srcColNames)+1, min_samples_split=3, class_weight='balanced'\n",
    "model = clf.fit(X_cal, y_cal)\n",
    "feature_importance = pd.DataFrame(data=model.feature_importances_.flatten(), index=srcColNames)\n",
    "print(feature_importance)\n",
    "\n",
    "#print(model.decision_path(X_cal))\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(random_state=0, max_depth=len(srcColNames)+1, min_samples_split=3) # \n",
    "\n",
    "X, y = df.loc[:, srcColNames].values, df.loc[:, tgtColName].values\n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [len(srcColNames), len(srcColNames)+1, None],\n",
    "              \"max_features\": sp_randint(3, len(srcColNames)-1),\n",
    "              \"min_samples_split\": sp_randint(2, 8),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=5)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\"max_depth\": [len(srcColNames), None],\n",
    "              \"max_features\": [3, len(srcColNames)-1],\n",
    "              \"min_samples_split\": [2, 3, 8],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(model, out_file=None, \n",
    "                         feature_names=srcColNames,  \n",
    "                         class_names=tgtColName,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True) \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "model = clf.fit(X_cal, y_cal)\n",
    "feature_importance = pd.DataFrame(data=model.feature_importances_.flatten(), index=srcColNames)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "calcRMS = lambda y_pred, y: np.sqrt(np.mean(np.power(y_pred - y, 2)))\n",
    "def predict(X, y, usage='CAL'):\n",
    "    y_pred = model.predict(X)\n",
    "    rms = calcRMS(y_pred, y)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    cm_norm = np.round(cm.astype('float') / cm.sum(axis=0).reshape((1,2)) * 100, 3)\n",
    "    print(\"Clf model rms on {} set: {}\".format(usage, rms))\n",
    "    print(\"Clf model confusion matrix on {} set:\\n{}\\n{}\".format(usage, cm, cm_norm))\n",
    "predict(X_cal, y_cal)\n",
    "predict(X_ver, y_ver, 'VER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "import sys\n",
    "import os.path\n",
    "sys.path.insert(0, os.getcwd()+\"/../../../../libs/tacx\")\n",
    "print(os.getcwd()+\"/../../../../libs/tacx\")\n",
    "from SEMContour import *\n",
    "sys.path.insert(0, os.getcwd()+\"/../../../../libs/common\")\n",
    "from FileUtil import gpfs2WinPath\n",
    "\n",
    "import glob\n",
    "\n",
    "CWD = ''.join(['/gpfs/WW/BD/MXP/SHARED/SEM_IMAGE/IMEC/Case02_calaveras_v3/3Tmp/CT_KPI_test/Calaveras_v3_regular_CT_KPI_003_slope_modified_revert_all_patterns/'\n",
    "      'h/cache/dummydb/result/MXP/job1/ContourSelectModelCalibration430result1'])\n",
    "\n",
    "''' # comment block 1 starts\n",
    "#################\n",
    "# type 1, review model apply image by random permutation\n",
    "#################\n",
    "pathfilter = '*_image_contour.txt'\n",
    "pathex = gpfs2WinPath(os.path.join(CWD, pathfilter))\n",
    "contourfiles = glob.glob(pathex)\n",
    "np.random.RandomState(128)\n",
    "contourindice = np.random.permutation(np.arange(len(contourfiles)))\n",
    "for ii in range(0*8, 1*8):\n",
    "    fig = plt.figure()\n",
    "    for jj, idx in enumerate(contourindice[ii*8:(ii+1)*8]):\n",
    "        contourfile = contourfiles[idx]\n",
    "        patternid = os.path.basename(contourfile).strip('_image_contour.txt')\n",
    "        ################# end of type 1\n",
    "''' # comment block 1 ends\n",
    "        \n",
    "#################\n",
    "# type 2, review model apply image by giving list\n",
    "#################\n",
    "patternids = [461, 1001]\n",
    "\n",
    "for ii in range(int(np.ceil(len(patternids)/8.))):\n",
    "    fig = plt.figure()\n",
    "    for jj, idx in enumerate(range(ii*8, (ii+1)*8)):\n",
    "        patternid = str(patternids[idx])\n",
    "        contourfile = gpfs2WinPath(os.path.join(CWD, patternid+'_image_contour.txt'))\n",
    "        ################# end of type 2        \n",
    "        \n",
    "        if not os.path.exists(contourfile):\n",
    "            print(patternid+' not exist')\n",
    "            continue\n",
    "\n",
    "        # get contour data\n",
    "        contour = SEMContour()\n",
    "        if not contour.parseFile(contourfile):\n",
    "            continue\n",
    "        df = contour.toDf()\n",
    "\n",
    "        X_test = df.loc[:, srcColNames].values\n",
    "        X_test = np.array([(X_test[:,i] - Xmin[i])/(Xmax[i] - Xmin[i]) for i in range(len(srcColNames)) ]).T\n",
    "        df.loc[:, 'ClfLabel'] = model.predict(X_test)\n",
    "        # SEM Contour Selection resulst plot: by classifer Positive 0, & Negative 1\n",
    "        def plotContourDiscriminator(contour, im=None, wndname=''):\n",
    "            # plot image and classified contour point\n",
    "            \n",
    "            ax = fig.add_subplot(2,4,jj+1)\n",
    "\n",
    "            imw, imh = contour.getshape()\n",
    "            ax.set_aspect('equal')\n",
    "            xini, yini, xend, yend = contour.getBBox()\n",
    "            ax.set_xlim([xini, xend])\n",
    "            ax.set_ylim([yini, yend])\n",
    "            ax.set_title(wndname)\n",
    "\n",
    "            df = contour.toDf()\n",
    "            Positive = df.ClfLabel==0\n",
    "            Negative = df.ClfLabel==1\n",
    "\n",
    "            # calculate confusion matrix\n",
    "            cm = np.array([len(df.loc[flt, :]) for flt in [Positive, Negative]])\n",
    "            cm_norm = cm.astype('float') / cm.sum(axis=1).reshape((2,1))\n",
    "\n",
    "            if im is not None:\n",
    "                ax.imshow(im)\n",
    "            ax.plot(df.loc[Positive ,'offsetx'], df.loc[Positive, 'offsety'], #'b.', markersize=1, \n",
    "                    linestyle='None', marker= 'o', markeredgecolor='r', markersize=2, markeredgewidth=1, markerfacecolor='none', \n",
    "                    label='remove: {}({:.3f}%)'.format(cm[0], cm_norm[0]*100 )) #Discriminator Positive, ClfLabel=0\n",
    "            ax.plot(df.loc[Negative ,'offsetx'], df.loc[Negative, 'offsety'], #'r*', markersize=2,\n",
    "                    linestyle='None', marker= '.', markeredgecolor='b', markersize=2, markeredgewidth=1, markerfacecolor='none', \n",
    "                    label='Keep: {}({:.3f}%)'.format(cm[1], cm_norm[1]*100 )) #Discriminator Negative, ClfLabel=1:\n",
    "\n",
    "            #ax = plt.gca() # gca() function returns the current Axes instance\n",
    "            #ax.set_ylim(ax.get_ylim()[::-1]) # reverse Y\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.legend(loc=1)\n",
    "            plt.show()\n",
    "        plotContourDiscriminator(contour.fromDf(df), wndname='Pattern '+ patternid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Note the problem is too easy: the hyperparameter plateau is too flat and the\n",
    "# output model is the same for precision and recall with ties in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
