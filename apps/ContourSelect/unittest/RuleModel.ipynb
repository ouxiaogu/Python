{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os.path\n",
    "sys.path.insert(0, os.getcwd()+\"/../../../libs/tacx\")\n",
    "from SEMContour import SEMContour\n",
    "sys.path.insert(0, os.getcwd()+\"/../../../libs/common\")\n",
    "from FileUtil import gpfs2WinPath\n",
    "\n",
    "CWD = '/gpfs/WW/BD/MXP/SHARED/SEM_IMAGE/IMEC/Case02_calaveras_v3/3Tmp/CT_KPI_test/Calaveras_v3_regular_CT_KPI_003_slope_modified_revert_all_patterns/h/cache/dummydb/result/MXP/job1/ContourSelectModelCalibration430result1'\n",
    "#CWD = r'C:\\Localdata\\D\\Note\\Python\\apps\\MXP\\ContourSelect\\samplejob\\h\\cache\\dummydb\\result\\MXP\\job1\\ContourSelectModelCalibration430result1'\n",
    "#CWD = r'C:\\Localdata\\D\\Note\\Python\\apps\\MXP\\ContourSelect\\samplejob1\\h\\cache\\dummydb\\result\\MXP\\job1\\ContourExtraction400result1'\n",
    "CWD = gpfs2WinPath(CWD)\n",
    "allNeighborColNames = ['NeighborContinuity', 'NeighborOrientation', 'NeighborParalism']\n",
    "\n",
    "class ContourAnalyzer(object):\n",
    "    \"\"\"docstring for ContourData\"\"\"\n",
    "    def __init__(self, contourfile):\n",
    "        self.__build(contourfile)\n",
    "\n",
    "    def __build(self, contourfile):\n",
    "        contour = SEMContour()\n",
    "        contour.parseFile(contourfile)\n",
    "        if not contour:\n",
    "            sys.exit(\"ERROR: read in cmntour file %s fails\\n\" % contourfile)\n",
    "        self.contour = contour\n",
    "        self.df = contour.toDf()\n",
    "# get contour data\n",
    "patternid = '2438'\n",
    "contourfile = os.path.join(CWD, patternid+'_image_contour.txt')\n",
    "ca = ContourAnalyzer(contourfile)\n",
    "df = ca.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot by column unique labels\n",
    "def plot_col_by_label(ca, patternid='', colname=''):\n",
    "    contour = ca.contour\n",
    "    df = ca.df\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    xini, yini, xend, yend = contour.getBBox()\n",
    "    ax.set_xlim([xini, xend])\n",
    "    ax.set_ylim([yini, yend])\n",
    "    ax.set_title(\"Pattern \"+patternid)\n",
    "    \n",
    "    uniqVals = df.loc[:, colname].drop_duplicates().values\n",
    "    print(uniqVals)\n",
    "    for label in uniqVals:\n",
    "        flt_eq = df.loc[:, colname] == label\n",
    "        if label == 'nan':\n",
    "            flt_eq = df.loc[:, colname].isna()\n",
    "        ax.plot(df.loc[flt_eq, 'offsetx'], df.loc[flt_eq, 'offsety'], '.', linestyle='None',  markersize=2, label=colname+'=={}'.format(label))\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNeighborFeatures(df):\n",
    "    '''\n",
    "    add Features for the input contour DataFrame, based on the neighbor relationship in the context of segment\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: [in, out] contour as DataFrame\n",
    "        [in] Contour df, must contains `polygonId`, `angle`, `offsetx`, `offsety`\n",
    "        [out] Contour df, added `NeighborContinuity`, `NeighborOrientation`, `NeighborParalism`\n",
    "\n",
    "            - `NeighborContinuity`:  |X(n) - X(n-1)|^2, usually is to 1 (because of 8-neighbor contour tracing)\n",
    "            - `NeighborOrientation`:  dot(EigenVector(n), EigenVector(n-1)), closer to 1, the better(may use 1-dot)\n",
    "            - `NeighborParalism`:  ||cross((X(n) - X(n-1)), EigenVector(n-1))||, closer to 1, the better(may use 1-cross)\n",
    "    Note, the segment neighborhood based features can only be obtained by the whole segment, can't use ROI cropped segment \n",
    "    '''\n",
    "    if len(df) <= 0:\n",
    "        return df\n",
    "    polygonIds = df.loc[:, 'polygonId'].drop_duplicates().values\n",
    "    preIdx = df.index[0]\n",
    "    for polygonId in polygonIds:\n",
    "        # search forward \n",
    "        neighborFeatureValForward = []\n",
    "        isPolygonHead = True\n",
    "        for curIdx in df.loc[df['polygonId']==polygonId, :].index:\n",
    "            NeighborContinuity = 1\n",
    "            NeighborOrientation = 1\n",
    "            NeighborParalism = 1\n",
    "            if not isPolygonHead:\n",
    "                eigenvector_n_1 = np.array([np.cos(df.loc[preIdx, 'angle']), np.sin(df.loc[preIdx, 'angle'])])\n",
    "                eigenvector_n = np.array([np.cos(df.loc[curIdx, 'angle']), np.sin(df.loc[curIdx, 'angle'])])\n",
    "                neighorvector = np.array([df.loc[curIdx, 'offsetx'] - df.loc[preIdx, 'offsetx'],\n",
    "                                        df.loc[curIdx, 'offsety'] - df.loc[preIdx, 'offsety']])\n",
    "                crossvector = np.cross(neighorvector, eigenvector_n_1)\n",
    "\n",
    "                NeighborContinuity = np.sqrt(neighorvector.dot(neighorvector))\n",
    "                NeighborOrientation = eigenvector_n.dot(eigenvector_n_1)\n",
    "                NeighborParalism = np.sqrt(crossvector.dot(crossvector))/NeighborContinuity\n",
    "                NeighborContinuity = NeighborContinuity\n",
    "            preIdx = curIdx\n",
    "            isPolygonHead = False\n",
    "            neighborFeatureValForward.append((NeighborContinuity, NeighborOrientation, NeighborParalism))\n",
    "\n",
    "        # search backward\n",
    "        neighborFeatureValBackward = []\n",
    "        isPolygonHead = True\n",
    "        for curIdx in df.loc[df['polygonId']==polygonId, :].index[::-1]:\n",
    "            NeighborContinuity = 1\n",
    "            NeighborOrientation = 1\n",
    "            NeighborParalism = 1\n",
    "            if not isPolygonHead:\n",
    "                eigenvector_n_1 = np.array([np.cos(df.loc[preIdx, 'angle']), np.sin(df.loc[preIdx, 'angle'])])\n",
    "                eigenvector_n = np.array([np.cos(df.loc[curIdx, 'angle']), np.sin(df.loc[curIdx, 'angle'])])\n",
    "                neighorvector = np.array([df.loc[curIdx, 'offsetx'] - df.loc[preIdx, 'offsetx'],\n",
    "                                        df.loc[curIdx, 'offsety'] - df.loc[preIdx, 'offsety']])\n",
    "                crossvector = np.cross(neighorvector, eigenvector_n_1)\n",
    "\n",
    "                NeighborContinuity = np.sqrt(neighorvector.dot(neighorvector))\n",
    "                NeighborOrientation = eigenvector_n.dot(eigenvector_n_1)\n",
    "                NeighborParalism = np.sqrt(crossvector.dot(crossvector))/NeighborContinuity\n",
    "                NeighborContinuity = NeighborContinuity\n",
    "            preIdx = curIdx\n",
    "            isPolygonHead = False\n",
    "            neighborFeatureValBackward.append((NeighborContinuity, NeighborOrientation, NeighborParalism))\n",
    "\n",
    "        # merge results\n",
    "        neighborFeatureValBackward = neighborFeatureValBackward[::-1]\n",
    "        for ii, curIdx in enumerate(df.loc[df['polygonId']==polygonId, :].index):\n",
    "            for jj, colname in enumerate(allNeighborColNames):\n",
    "                '''\n",
    "                if ii == 0 or ii == len(neighborFeatureValForward)-1:\n",
    "                    if abs(neighborFeatureValForward[ii][jj] - 1) >= abs(neighborFeatureValBackward[ii][jj] - 1):\n",
    "                        val = neighborFeatureValForward[ii][jj]\n",
    "                    else:\n",
    "                        val = neighborFeatureValBackward[ii][jj]\n",
    "                else:\n",
    "                    val = 0.5*(neighborFeatureValForward[ii][jj] + neighborFeatureValBackward[ii][jj])\n",
    "                '''\n",
    "                if abs(neighborFeatureValForward[ii][jj] - 1) >= abs(neighborFeatureValBackward[ii][jj] - 1):\n",
    "                    val = neighborFeatureValForward[ii][jj]\n",
    "                else:\n",
    "                    val = neighborFeatureValBackward[ii][jj]\n",
    "                df.loc[curIdx, colname] = val\n",
    "    return df\n",
    "\n",
    "def plot_multi_filters(ca, patternid='', strFlts=None, transform_filter=False):\n",
    "    if strFlts is None:\n",
    "        newStrFlts = {}\n",
    "    elif not isinstance(strFlts, dict):\n",
    "        newStrFlts = {}\n",
    "        for col in allNeighborColNames:\n",
    "            for strFlt in strFlts:\n",
    "                if col in strFlt:\n",
    "                    newStrFlts[col] = strFlt\n",
    "                    break\n",
    "    strFlts = newStrFlts\n",
    "    print(strFlts)\n",
    "    df = ca.df\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    xini, yini, xend, yend = ca.contour.getBBox()\n",
    "    ax.set_xlim([xini, xend])\n",
    "    ax.set_ylim([yini, yend])\n",
    "    ax.set_title(\"Pattern \"+patternid)\n",
    "    \n",
    "    # plot contour\n",
    "    ax.plot(df.loc[:, 'offsetx'], df.loc[:, 'offsety'], 'k.', markersize=1, label='SEM Contour')\n",
    "    # plot angle\n",
    "    for _, row in df.iterrows():\n",
    "        x, y = row.loc['offsetx'], row.loc['offsety']\n",
    "        angle = row.loc['angle']\n",
    "        arrow_length = 1\n",
    "        dx, dy = arrow_length*np.cos(angle), arrow_length*np.sin(angle)\n",
    "        ax.arrow(x, y, dx, dy, width=0.1, fc='y', ec='y') # ,shape='right', overhang=0\n",
    "\n",
    "    # plot filters\n",
    "    if not transform_filter:\n",
    "        for strFlt in strFlts.values():\n",
    "            curdf = df.query(strFlt)\n",
    "            ax.plot(curdf.loc[:, 'offsetx'], curdf.loc[:, 'offsety'], 'o', markersize=4, label=strFlt, alpha=0.6)\n",
    "    else:\n",
    "        '''\n",
    "        inflection_points = []\n",
    "        for strFlt in strFlts.values():\n",
    "            curdf = df.query(strFlt )\n",
    "            print(strFlt, len(curdf))\n",
    "            if len(curdf) != 0:\n",
    "                inflection_points.append(curdf)\n",
    "        inflection_df = pd.concat(inflection_points)\n",
    "        '''\n",
    "        inflection_df = df.query('or '.join(strFlts.values()))\n",
    "        print(inflection_df[['polygonId', 'offsetx', 'offsety'] + allNeighborColNames[1:]])\n",
    "        \n",
    "        polygonIds = inflection_df.loc[:, 'polygonId'].drop_duplicates().values\n",
    "        for polygonId in polygonIds:\n",
    "            curdf = inflection_df.loc[inflection_df['polygonId']==polygonId, :]\n",
    "            maxNeighborContinuity = curdf.max()['NeighborContinuity']\n",
    "            minNeighborContinuity, minNeighborOrientation, minNeighborParalism = curdf.min()[allNeighborColNames]\n",
    "\n",
    "            if minNeighborParalism < minNeighborOrientation and len(curdf.query(strFlts['NeighborParalism'])) > 0:\n",
    "                idxmin = curdf['NeighborParalism'].idxmin()\n",
    "                ax.plot(curdf.loc[idxmin, 'offsetx'], curdf.loc[idxmin, 'offsety'], 'ro', markersize=4, label='minNeighborParalism', alpha=0.6)\n",
    "            elif minNeighborOrientation < minNeighborParalism and len(curdf.query(strFlts['NeighborOrientation'])) > 0:\n",
    "                idxmin = curdf['NeighborOrientation'].idxmin()\n",
    "                ax.plot(curdf.loc[idxmin, 'offsetx'], curdf.loc[idxmin, 'offsety'], 'rd', markersize=4, label='minNeighborOrientation', alpha=0.6)\n",
    "            elif len(curdf.query(strFlts['NeighborContinuity'])) > 0:\n",
    "                if abs(maxNeighborContinuity-1) > abs(minNeighborContinuity-1):\n",
    "                    idxmax = curdf['NeighborContinuity'].idxmax()\n",
    "                    ax.plot(curdf.loc[idxmax, 'offsetx'], curdf.loc[idxmax, 'offsety'], 'bd', markersize=4, label='maxNeighborContinuity', alpha=0.6)\n",
    "                else:\n",
    "                    idxmin = curdf['NeighborContinuity'].idxmin()\n",
    "                    ax.plot(curdf.loc[idxmin, 'offsetx'], curdf.loc[idxmin, 'offsety'], 'bo', markersize=4, label='minNeighborContinuity', alpha=0.6)\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return inflection_df\n",
    "    \n",
    "    #inflection_df.plot.scatter(x=allNeighborColNames[1], y=allNeighborColNames[2])\n",
    "    #plt.show()\n",
    "#inflection_df = plot_multi_filters(ca, patternid=patternid, strFlts=['abs(1-NeighborContinuity) > 0.5', 'NeighborParalism<0.98', 'NeighborOrientation<0.98'], transform_filter=True)\n",
    "df = addNeighborFeatures(df)\n",
    "ca.df = df\n",
    "inflection_df = plot_multi_filters(ca, patternid=patternid, strFlts=['abs(1-NeighborContinuity) > 0.5', 'NeighborParalism<0.98', 'NeighborOrientation<0.98'], transform_filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMeanOfLargestHistBin(arr, bins=10):\n",
    "    hist, bin_edges = np.histogram(arr, bins=bins)\n",
    "    idxmax = np.argmax(hist)\n",
    "    binvals = arr[np.where(np.logical_and(arr>=bin_edges[idxmax], arr<bin_edges[idxmax+1]))]\n",
    "    return np.mean(binvals)\n",
    "\n",
    "def findIndexOfFirstFlat(arr, gradients=None, start_pos=0, thres=None):\n",
    "    if gradients is None:\n",
    "        gradients = np.gradient(arr, edge_order=2)\n",
    "    assert(len(arr) == len(gradients))\n",
    "    absGradients = np.abs(gradients)\n",
    "    if thres is None:\n",
    "        thres = calcMeanOfLargestHistBin(absGradients)\n",
    "    for ix in range(start_pos, len(gradients)):\n",
    "        if absGradients[ix] < thres:\n",
    "            print(thres, ix)\n",
    "            return ix\n",
    "    return start_pos\n",
    "\n",
    "def findIndexOfFirstZeroCrossing(arr, gradients=None, start_pos=0):\n",
    "    if gradients is None:\n",
    "        gradients = np.gradient(arr, edge_order=2)\n",
    "    assert(len(arr) == len(gradients))\n",
    "    start = start_pos+1 if start_pos == 0 else start_pos\n",
    "    for ix in range(start_pos, len(gradients)):\n",
    "        if gradients[ix]*gradients[ix-1] <=0 :\n",
    "            return ix\n",
    "    return start_pos\n",
    "\n",
    "\n",
    "flt = [0.10650698, 0.78698604, 0.10650698] # sigma = 0.5\n",
    "#flt = [0.17, 0.66, 0.17] # sigma 0.6\n",
    "#flt = [0.25, 0.5, 0.25] # sigma 0.8\n",
    "print(flt)\n",
    "def smoothSignal(arr):\n",
    "    padArr = np.zeros((arr.shape[0]+2,) )\n",
    "    padArr[0] = arr[0]\n",
    "    padArr[1:-1] = arr\n",
    "    padArr[-1] = arr[-1]\n",
    "    '''\n",
    "    arr = list(arr)\n",
    "    padArr = arr[0:1] + arr[:] + arr[-1:]\n",
    "    '''\n",
    "    newArr = np.convolve(padArr, flt, 'valid')\n",
    "    if len(arr) != len(newArr):\n",
    "        raise ValueError(\"inequal length {} {}\".format(len(arr), len(newArr)))\n",
    "    return newArr\n",
    "\n",
    "#vlines = inflection_df.groupby(['polygonId'])\n",
    "for polygonId in [29]: \n",
    "    print(polygonId)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    curdf = df.loc[df.polygonId==polygonId, :]\n",
    "    x = np.arange(len(curdf))\n",
    "    arr1 = curdf[allNeighborColNames[1]].values\n",
    "    arr2 = curdf[allNeighborColNames[2]].values\n",
    "    #arr2 = smoothSignal(arr2)\n",
    "    gradient1 = np.gradient(arr1, edge_order=2)\n",
    "    gradient2 = np.gradient(arr2, edge_order=2)\n",
    "    Paralism_xx = np.gradient(gradient2, edge_order=2)\n",
    "    \n",
    "    idxmin = np.argmin(arr2)\n",
    "    start_pos = min(idxmin+1, len(arr2)-1)\n",
    "    ax.plot(idxmin, arr2[idxmin], 'o', markersize=12, markeredgewidth=2, markerfacecolor='none', label= 'min'+allNeighborColNames[2])\n",
    "    idx1 = findIndexOfFirstFlat(arr2, gradient2, start_pos=start_pos)\n",
    "    idx2 = findIndexOfFirstZeroCrossing(arr2, gradient2, start_pos=start_pos)\n",
    "    ax.plot(idx1, arr2[idx1], 'o', markersize=12, markeredgewidth=2, markerfacecolor='none', label= allNeighborColNames[2] + ' zero gradient')\n",
    "    ax.plot(idx2, arr2[idx2], 'o', markersize=12, markeredgewidth=2, markerfacecolor='none', label= allNeighborColNames[2] + ' zero-crossing gradient')\n",
    "    \n",
    "    \n",
    "    #ax.plot(x, (curdf[allNeighborColNames[0]]-1).abs(), 'g-d', label=allNeighborColNames[0])\n",
    "    #ax.plot(x, curdf[allNeighborColNames[1]], '-o', label= allNeighborColNames[1])\n",
    "    #ax.plot(x, curdf[allNeighborColNames[2]], '--o', label= allNeighborColNames[2], alpha=0.6)\n",
    "    ax.plot(x, arr2, '--o', label= allNeighborColNames[2], alpha=0.6)\n",
    "    #ax.plot(x, gradient1, '-*', label= allNeighborColNames[1] + ' gradient')\n",
    "    #ax.plot(x, gradient2, '--*', label= allNeighborColNames[2] + ' gradient')\n",
    "    #ax.plot(x, Paralism_xx, '--d', label= allNeighborColNames[2] + ' 2nd deriative')\n",
    "    \n",
    "    if idxmin in range(40):\n",
    "        ax.set_xlim([0, 40])\n",
    "    #plt.yscale('log')    \n",
    "    ax.set_title(\"Pattern {} Vline {}\".format(patternid, polygonId))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outColName='ClfLabel'\n",
    "\n",
    "def categorizeFilters(filters):\n",
    "    if filters is None:\n",
    "        newfilters = {}\n",
    "    elif not isinstance(filters, dict):\n",
    "        newfilters = {}\n",
    "        for col in allNeighborColNames:\n",
    "            for strFlt in filters:\n",
    "                if col in strFlt:\n",
    "                    newfilters[col] = strFlt\n",
    "                    break\n",
    "    filters = newfilters\n",
    "    # print(filters)\n",
    "    return filters\n",
    "\n",
    "def applyNeighborRuleModelPerVLine(linedf, filters=None, maxTailLength=20, smooth=True):\n",
    "    dominant_issues = []\n",
    "    linedf.loc[:, outColName] = 1\n",
    "\n",
    "    # step 1, search and apply from head\n",
    "    headLength = min(len(linedf), maxTailLength)\n",
    "    headdf = linedf.loc[linedf.index[:headLength], :]\n",
    "    minNeighborOrientation, minNeighborParalism = headdf.min()[allNeighborColNames[1:]]\n",
    "    issue_feature, issue_index = None, None\n",
    "    if minNeighborParalism < minNeighborOrientation and len(headdf.query(filters['NeighborParalism'])) > 0:\n",
    "        issue_feature = 'NeighborParalism'\n",
    "        issue_index = np.argmin(headdf[issue_feature].values)\n",
    "    elif minNeighborOrientation < minNeighborParalism and len(headdf.query(filters['NeighborOrientation'])) > 0:\n",
    "        issue_feature = 'NeighborOrientation'\n",
    "        issue_index = np.argmin(headdf[issue_feature].values)\n",
    "    dominant_issues.append(None)\n",
    "    if issue_feature is not None:\n",
    "        dominant_issues[0] = [issue_feature, issue_index]\n",
    "        arr = linedf.loc[:, issue_feature].values\n",
    "        if smooth:\n",
    "            arr = smoothSignal(arr)\n",
    "        gradient = np.gradient(arr, edge_order=2)\n",
    "        start_pos = min(issue_index+1, len(arr)-1)\n",
    "        idxFlat = findIndexOfFirstFlat(arr, gradient, start_pos=start_pos)\n",
    "        dominant_issues[0].append(idxFlat)\n",
    "        linedf.loc[linedf.index[:idxFlat], outColName] = 0\n",
    "\n",
    "    # step 2, search and apply from tail, reverse order\n",
    "    dominant_issues.append(None)\n",
    "    head_index = 0 if issue_index is None else issue_index\n",
    "    tailrange = len(linedf) - (head_index + 1) # exclude the head issue index itself\n",
    "    if tailrange > 0:\n",
    "        tailLength = min(tailrange, maxTailLength)\n",
    "        taildf = linedf.loc[linedf.index[-tailLength:], :]\n",
    "        minNeighborOrientation, minNeighborParalism = taildf.min()[allNeighborColNames[1:]]\n",
    "        issue_feature, issue_index = None, None\n",
    "        if minNeighborParalism < minNeighborOrientation and len(taildf.query(filters['NeighborParalism'])) > 0:\n",
    "            issue_feature = 'NeighborParalism'\n",
    "            issue_index = np.argmin(taildf[issue_feature].values)\n",
    "            issue_index = tailLength - 1 - issue_index  # use index start from tail\n",
    "        elif minNeighborOrientation < minNeighborParalism and len(taildf.query(filters['NeighborOrientation'])) > 0:\n",
    "            issue_feature = 'NeighborOrientation'\n",
    "            issue_index = np.argmin(taildf[issue_feature].values)\n",
    "            issue_index = tailLength - 1 - issue_index\n",
    "        if issue_feature is not None:\n",
    "            dominant_issues[1] = [issue_feature, issue_index]\n",
    "            arr = linedf.loc[:, issue_feature].values[::-1]\n",
    "            if smooth:\n",
    "                arr = smoothSignal(arr)\n",
    "            gradient = np.gradient(arr, edge_order=2)\n",
    "            start_pos = min(issue_index+1, len(arr)-1)\n",
    "            idxFlat = findIndexOfFirstFlat(arr, gradient, start_pos=start_pos)\n",
    "            dominant_issues[1].append(idxFlat)\n",
    "            linedf.loc[linedf.index[-idxFlat:], outColName] = 0\n",
    "    return linedf, dominant_issues\n",
    "\n",
    "def applyNeighborRuleModel(contourdf, filters, smooth=True):\n",
    "    '''\n",
    "    The step to find rule model in python:\n",
    "    1. apply combined filters to find ill contour Vline candidates\n",
    "    2. find the Vline candidates have dominant issues in its head+20 or tail-20\n",
    "    3. remove contour issue head/tail by following rules(default is 3.1):\n",
    "        * 3.1: search start from dominant issue position, new head=Index[1st flat gradient point]\n",
    "        * 3.2: Index[dominant issue position], new head=Index[the gradient zero-crossing point]\n",
    "    '''\n",
    "    maxTailLenth = 20\n",
    "    contourdf.loc[:, 'ClfLabel'] = 1\n",
    "    filters = categorizeFilters(filters)\n",
    "\n",
    "    inflection_df = contourdf.query('or '.join(filters.values()))\n",
    "    # print(inflection_df[['polygonId', 'offsetx', 'offsety'] + allNeighborColNames[1:]])\n",
    "    polygonIds = inflection_df.loc[:, 'polygonId'].drop_duplicates().values\n",
    "    for polygonId in polygonIds:\n",
    "        lineFlt = contourdf['polygonId']==polygonId\n",
    "        linedf = contourdf.loc[lineFlt, :]\n",
    "        newlinedf, dominant_issues = applyNeighborRuleModelPerVLine(linedf, filters, maxTailLenth, smooth)\n",
    "        print(int(polygonId), dominant_issues)\n",
    "        contourdf.loc[lineFlt, :] = newlinedf\n",
    "    return contourdf\n",
    "\n",
    "df = applyNeighborRuleModel(df, ['NeighborOrientation<0.98', 'NeighborParalism<0.98'])\n",
    "ca.df = df\n",
    "plot_col_by_label(ca, patternid=patternid, colname='ClfLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SEM Contour Selection resulst plot: by classifer Positive 0, & Negative 1\n",
    "def plotContourDiscriminator(contour, im=None, fig=None, subidx=111, wndname=''):\n",
    "    # plot image and classified contour point\n",
    "    df = contour.toDf()\n",
    "    if any([col not in df.columns for col in ['ClfLabel']]):\n",
    "        return False\n",
    "\n",
    "    # fig = plt.figure()\n",
    "    if subidx >= 111:\n",
    "        ax = fig.add_subplot(subidx)\n",
    "    else:\n",
    "        ax = fig.add_subplot(2, 4, subidx)\n",
    "    \n",
    "    imw, imh = contour.getshape()\n",
    "    '''\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim([0, imw])\n",
    "    ax.set_ylim([0, imh])\n",
    "    '''\n",
    "    xini, yini, xend, yend = contour.getBBox()\n",
    "    ax.set_xlim([xini, xend])\n",
    "    ax.set_ylim([yini, yend])\n",
    "    ax.set_title(wndname)\n",
    "    \n",
    "    Positive = df.ClfLabel==0\n",
    "    Negative = df.ClfLabel==1\n",
    "\n",
    "    # calculate confusion matrix\n",
    "    cm = np.array([len(df.loc[flt, :]) for flt in [Positive, Negative]])\n",
    "    cm_norm = cm.astype('float') / cm.sum()\n",
    "    \n",
    "    if im is not None:\n",
    "        if len(im.shape) == 2:\n",
    "            im = cv2.cvtColor((im/65536.).astype('float32'), cv2.COLOR_GRAY2BGR)\n",
    "        ax.imshow(im)\n",
    "    ax.plot(df.loc[Positive ,'offsetx'], df.loc[Positive, 'offsety'], #'b.', markersize=1, \n",
    "        linestyle='None', marker= 'o', markeredgecolor='r', markersize=2, markeredgewidth=1, markerfacecolor='none',\n",
    "        label='remove: {}({:.3f}%)'.format(cm[0], cm_norm[0]*100 )) #Discriminator Positive, ClfLabel=0\n",
    "    ax.plot(df.loc[Negative ,'offsetx'], df.loc[Negative, 'offsety'], #'r*', markersize=2,\n",
    "        linestyle='None', marker= '.', markeredgecolor='b', markersize=2, markeredgewidth=1, markerfacecolor='none', \n",
    "        label='Keep: {}({:.3f}%)'.format(cm[1], cm_norm[1]*100 )) #Discriminator Negative, ClfLabel=1:\n",
    "    \n",
    "    #ax = plt.gca() # gca() function returns the current Axes instance\n",
    "    #ax.set_ylim(ax.get_ylim()[::-1]) # reverse Y\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend(loc=1)\n",
    "    plt.show()\n",
    "    return True\n",
    "\n",
    "def plotComparedContourResults():\n",
    "\n",
    "    sys.path.insert(0, os.getcwd()+\"/../../../libs/tacx\")\n",
    "    print(os.getcwd()+\"/../../../libs/tacx\")\n",
    "    sys.path.insert(0, os.getcwd()+\"/../../../libs/common\")\n",
    "    \n",
    "    jobpath = '/gpfs/WW/BD/MXP/SHARED/SEM_IMAGE/IMEC/Case02_calaveras_v3/3Tmp/CT_KPI_test/Calaveras_v3_regular_CT_KPI_003_slope_modified_revert_all_patterns/'\n",
    "    resultpaths = [\n",
    "          'h/cache/dummydb/result/MXP/job1/ContourSelectModelCalibration430result1', # clf \n",
    "          'h/cache/dummydb/result/MXP/job1/ContourSelectModelCalibration431result1', # rule\n",
    "          ]\n",
    "    impath = 'h/cache/dummydb/result/MXP/job1/Average301result1'\n",
    "    modeltypes = ['clf', 'rule']\n",
    "    CWDs = [''.join([jobpath, path]) for path in resultpaths]\n",
    "    impath = ''.join([jobpath, impath])\n",
    "    \n",
    "    ''' # comment block 1 starts\n",
    "    #################\n",
    "    # type 1, review model apply image by random permutation\n",
    "    #################\n",
    "    pathfilter = '*_image_contour.txt'\n",
    "    pathex = gpfs2WinPath(os.path.join(CWD, pathfilter))\n",
    "    contourfiles = glob.glob(pathex)\n",
    "    contourindice = np.random.permutation(np.arange(len(contourfiles)))\n",
    "    for ii in range(0*8, 1*8):\n",
    "        fig = plt.figure()\n",
    "        for jj, idx in enumerate(contourindice[ii*8:(ii+1)*8]):\n",
    "            contourfile = contourfiles[idx]\n",
    "            patternid = os.path.basename(contourfile).strip('_image_contour.txt')\n",
    "            ################# end of type 1\n",
    "    ''' # comment block 1 ends\n",
    "            \n",
    "    #################\n",
    "    # type 2, review model apply image by giving list\n",
    "    #################\n",
    "    #patternids = [461, 1001]\n",
    "    #patternids = [118, 430, 432, 437, 442, 449, 461, 530, 536, 539, 542, 833, 1593, 1785, 1793, 1801, 1809, 2163, 2196, 2284, 2405, 2427, 2438, 2543, 2585, 2655, 2697, 2767, 3223, 3418, 3463, 3553, 3583, 3613, 3628]\n",
    "    #patternids = [432, 442, 449, 536, 1785, 2405, 2427, 2438,  2767, 2697, 3418, 3583, 3613, 49, 63, 438, 439, 3741,  3742]\n",
    "    patternids = [449, 1785, 2405, 2438,2767, 3418, 3583]\n",
    "\n",
    "    \n",
    "    psteps = 4 # pattern steps in the same outxml file\n",
    "    for ii in range(int(np.ceil(len(patternids)/float(psteps)))):\n",
    "        fig = plt.figure()\n",
    "        for jj, idx in enumerate(range(ii*psteps, (ii+1)*psteps)):\n",
    "            if idx > len(patternids):\n",
    "                return\n",
    "            for kk, CWD in enumerate(CWDs):\n",
    "                try:\n",
    "                    patternid = str(patternids[idx])\n",
    "                except IndexError:\n",
    "                    raise IndexError(patternid)\n",
    "                contourfile = gpfs2WinPath(os.path.join(CWD, patternid+'_image_contour.txt'))\n",
    "                ################# end of type 2        \n",
    "                \n",
    "                if not os.path.exists(contourfile):\n",
    "                    print(patternid+' not exist')\n",
    "                    continue\n",
    "        \n",
    "                # get contour data\n",
    "                contour = SEMContour()\n",
    "                if not contour.parseFile(contourfile):\n",
    "                    continue\n",
    "                if kk == 1:\n",
    "                    df = contour.toDf()\n",
    "                    df = addNeighborFeatures(df)\n",
    "                    df = applyNeighborRuleModel(df, ['NeighborOrientation<0.98', 'NeighborParalism<0.98'])\n",
    "                    contour = contour.fromDf(df)\n",
    "                #im, _ = imread_gray(os.path.join(impath, patternid+'_image.pgm'))\n",
    "                \n",
    "                plotContourDiscriminator(contour, fig=fig, subidx=kk*psteps+jj+1, wndname=modeltypes[kk]+' model Pattern '+ patternid)\n",
    "        \n",
    "plotComparedContourResults()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
