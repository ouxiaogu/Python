\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[latin1]{inputenc}

\title{Decision Tree Equations}
% \author{Veloci Raptor}
% \date{03/14/15}

\begin{document}
\maketitle

1. (Empirical) Entropy

\begin{align*}
  H(X) &= H(p) = -\sum_{i=1}^{n} p_i \log p_i \\
  H(D) &= -\sum_{k=1}^K \frac{|C_k|}{|D|} \log_2\frac{|C_k|}{|D|}
\end{align*}

2. (Empirical) Conditional Entropy


\begin{align*}
  H(Y|X)&=\sum_{i=1}^n p_i H(Y|X=x_i)  & p_i=P(X=x_i),i=1,2,\dots ,n \\
  H(D|A)&=\sum_{i=1}^n \frac{|D_i|}{|D|} H(D_i)  & i=1,2,\dots ,n \\
        &=-\sum_{i=1}^n \frac{|D_i|}{|D|} \sum_{k=1}^K \frac{|D_{ik}|}{|D_i|} \log_2\frac{|D_{ik}|}{|D_i|}
\end{align*}

3. Info Gain

$$ g(D,A) = H(D) - H(D|A) $$

4. Normalized Info Gain

$$
  g_R(D,A) = \frac {g(D, A)} {H(D)} = \frac {H(D) - H(D|A)} {H(D)}
$$

5. DT overall cost function


\begin{align*}
  C_\alpha(T) &= C(T) + \alpha|T| \\
    &=\sum_{i=1}^{|T|}N_tH_t(T)+\alpha|T| \\
    &=-\sum_{t=1}^{|T|}\sum_{k=1}^K N_{tk} \log\frac{N_{tk}}{N_t} + \alpha|T|
\end{align*}


\end{document}
