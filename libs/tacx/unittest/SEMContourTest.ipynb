{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os.path\n",
    "sys.path.insert(0, os.getcwd()+\"/..\")\n",
    "from SEMContour import *\n",
    "sys.path.insert(0, os.getcwd()+\"/../../common\")\n",
    "from FileUtil import gpfs2WinPath\n",
    "\n",
    "CWD = '/gpfs/WW/BD/MXP/SHARED/SEM_IMAGE/IMEC/Case02_calaveras_v3/3Tmp/CT_KPI_test/Calaveras_v3_regular_CT_KPI_003_slope_modified_revert_all_patterns/h/cache/dummydb/result/MXP/job1/ContourSelectModelCalibration430result1'\n",
    "#CWD = r'C:\\Localdata\\D\\Note\\Python\\apps\\MXP\\ContourSelect\\samplejob\\h\\cache\\dummydb\\result\\MXP\\job1\\ContourSelectModelCalibration430result1'\n",
    "#CWD = r'C:\\Localdata\\D\\Note\\Python\\apps\\MXP\\ContourSelect\\samplejob1\\h\\cache\\dummydb\\result\\MXP\\job1\\ContourExtraction400result1'\n",
    "CWD = gpfs2WinPath(CWD)\n",
    "allNeighborColNames = ['NeighborContinuity', 'NeighborOrientation', 'NeighborParalism']\n",
    "\n",
    "class ContourAnalyzer(object):\n",
    "    \"\"\"docstring for ContourData\"\"\"\n",
    "    def __init__(self, contourfile):\n",
    "        self.__build(contourfile)\n",
    "\n",
    "    def __build(self, contourfile):\n",
    "        contour = SEMContour()\n",
    "        contour.parseFile(contourfile)\n",
    "        if not contour:\n",
    "            sys.exit(\"ERROR: read in contour file %s fails\\n\" % contourfile)\n",
    "        self.contour = contour\n",
    "        self.df = contour.toDf()\n",
    "# get contour data\n",
    "patternid = '2438'\n",
    "contourfile = os.path.join(CWD, patternid+'_image_contour.txt')\n",
    "ca = ContourAnalyzer(contourfile)\n",
    "df = ca.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get contour data\n",
    "contourfile = os.path.join(CWD, '461_image_contour.txt')\n",
    "ca = ContourAnalyzer(contourfile)\n",
    "df = ca.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the SEM contour and angle\n",
    "def plot_contour_angle(ca, patternid='', arrow_length=1):\n",
    "    df = ca.df\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    imw, imh = ca.contour.getshape()\n",
    "    ax.set_xlim([0, imw])\n",
    "    ax.set_ylim([0, imh])\n",
    "    ax.set_title(\"Pattern \"+patternid+ \" image Contour\")\n",
    "    \n",
    "    # plot image\n",
    "    \n",
    "    # plot contour\n",
    "    ax.plot(df.loc[:, 'offsetx'], df.loc[:, 'offsety'], 'b.')\n",
    "    \n",
    "    # plot angle\n",
    "    for _, row in df.iterrows():\n",
    "        x, y = row.loc['offsetx'], row.loc['offsety']\n",
    "        angle = row.loc['angle']\n",
    "        dx, dy = arrow_length*np.cos(angle), arrow_length*np.sin(angle)\n",
    "        ax.arrow(x, y, dx, dy, width=0.1, fc='y', ec='y') # ,shape='right', overhang=0\n",
    "        \n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "plot_contour_angle(ca, '461')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'polygonId', u'offsetx', u'offsety', u'angle', u'weight',\n",
      "       u'confidence', u'intensity', u'slope', u'band_width',\n",
      "       u'ridge_intensity', u'curvature', u'contrast', u'mxp_flag',\n",
      "       u'EigenRatio', u'NeighborOrientation', u'NeighborParalism',\n",
      "       u'ClfLabel'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# plot the histgram for the modified slope, & plot by filter\n",
    "print(df.columns)\n",
    "colname = 'NeighborOrientation' # NeighborParalism\n",
    "df[colname].plot.hist(bins=100)\n",
    "def plot_col_filter(ca, patternid='', colname=''):\n",
    "    df = ca.df\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    imw, imh = ca.contour.getshape()\n",
    "    ax.set_xlim([0, imw])\n",
    "    ax.set_ylim([0, imh])\n",
    "    ax.set_title(\"Pattern \"+patternid)\n",
    "    \n",
    "    thresh = 0.99\n",
    "    flt_gt = df.loc[:, colname] > thresh\n",
    "    flt_eq = df.loc[:, colname] == thresh\n",
    "    flt_lt = df.loc[:, colname] < thresh\n",
    "    \n",
    "    ax.plot(df.loc[flt_gt, 'offsetx'], df.loc[flt_gt, 'offsety'], 'b.', markersize=2, label=colname+'>{}'.format(thresh))\n",
    "    ax.plot(df.loc[flt_eq, 'offsetx'], df.loc[flt_eq, 'offsety'], 'r.', markersize=2, label=colname+'=={}'.format(thresh))\n",
    "    ax.plot(df.loc[flt_lt, 'offsetx'], df.loc[flt_lt, 'offsety'], 'r.', markersize=2, label=colname+'<{}'.format(thresh))\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_col_filter(ca, patternid='461', colname=colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot by column unique labels\n",
    "def plot_col_by_label(ca, patternid='', colname=''):\n",
    "    contour = ca.contour\n",
    "    df = ca.df\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    xini, yini, xend, yend = contour.getBBox()\n",
    "    ax.set_xlim([xini, xend])\n",
    "    ax.set_ylim([yini, yend])\n",
    "    ax.set_title(\"Pattern \"+patternid)\n",
    "    \n",
    "    uniqVals = df.loc[:, colname].drop_duplicates().values\n",
    "    print(uniqVals)\n",
    "    for label in uniqVals:\n",
    "        flt_eq = df.loc[:, colname] == label\n",
    "        if label == 'nan':\n",
    "            flt_eq = df.loc[:, colname].isna()\n",
    "        ax.plot(df.loc[flt_eq, 'offsetx'], df.loc[flt_eq, 'offsety'], '.', linestyle='None',  markersize=2, label=colname+'=={}'.format(label))\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot contour filtering by ridge_intensity \n",
    "def plot_rd_filter(ca, patternid=''):\n",
    "    df = ca.df\n",
    "    imw, imh = ca.contour.getshape()\n",
    "    \n",
    "    figw = 9\n",
    "    fig = plt.figure(figsize=(figw, figw))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim([0, imw])\n",
    "    ax.set_ylim([0, imh])\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    thresh = 0.003\n",
    "    flt = df.loc[:, 'ridge_intensity'] > thresh\n",
    "    ax.plot(df.loc[flt, 'offsetx'], df.loc[flt, 'offsety'], 'g.', markersize=2, label='ridge_intensity>{}'.format(thresh))\n",
    "    ax.plot(df.loc[~flt, 'offsetx'], df.loc[~flt, 'offsety'], 'r.', markersize=3, label='ridge_intensity<={}'.format(thresh))\n",
    "    \n",
    "    ax.set_title(patternid+\" Rg>{} filter\".format(thresh))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_rd_filter(ca, 'Pattern 3658')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for pop out plot\n",
    "%matplotlib qt5\n",
    "\n",
    "def plot_reg(ca, winname=''):\n",
    "    df = ca.df\n",
    "    imw, imh = ca.contour.getshape()\n",
    "    \n",
    "    colstr = 'slope  ridge_intensity'\n",
    "    cols = colstr.split()\n",
    "    #df = df[cols]\n",
    "    df.loc[:, 'slope'] = df.loc[:, 'slope'].abs().values\n",
    "    ## df = df.loc[df.slope<0.03, :]\n",
    "    x, y = df.loc[:, 'slope'], df.loc[:, 'ridge_intensity']\n",
    "\n",
    "    # from scipy import stats\n",
    "    # slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "    #plt.plot(x, intercept + slope*x, 'r', label='fitted ridge_intensity')\n",
    "    import statsmodels.api as sm\n",
    "    X = sm.add_constant(x, prepend=False)\n",
    "    results = sm.OLS(y, X).fit()\n",
    "    # print(results.summary())\n",
    "    # print(results.mse_resid, results.mse_total)\n",
    "    # print(results.params, type(results.params))\n",
    "    k, b = results.params.loc['slope'], results.params.loc['const']\n",
    "    \n",
    "    from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "    pred_std, predict_ci_low, predict_ci_upp = wls_prediction_std(results)\n",
    "\n",
    "    xmax, ymax = x.max(), y.max()\n",
    "    figw = 7\n",
    "    fig = plt.figure(figsize=(figw, figw*ymax/xmax))\n",
    "    #fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlim([0, xmax*1.1])\n",
    "    ax.set_ylim([0, ymax*1.1])\n",
    "    \n",
    "    ax.set_title(winname+' ridge_intensity v.s. abs of modified slope')\n",
    "    ax.set_xlabel('modified slope')\n",
    "    ax.set_ylabel('ridge_intensity')\n",
    "    \n",
    "    ax.plot(x, y, 'o', label='original ridge_intensity v.s. slope')\n",
    "    y_pred = results.predict()\n",
    "    ax.plot(x, y_pred, 'r', label='predicted ridge_intensity={:.3f}slope+{:.3f}, $R^2={:.3f}$'.format(k, b, results.rsquared))\n",
    "    plt.plot(x, predict_ci_low, 'b--', lw=1, label='predict lower')\n",
    "    plt.plot(x, predict_ci_upp, 'g--', lw=1, label='predict upper')\n",
    "    \n",
    "    df.loc[:, 'predict_ci_low'] = predict_ci_low\n",
    "    df.loc[:, 'predict_ci_upp'] = predict_ci_upp\n",
    "    \n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    ## ridge_intensity prediction boundary plot\n",
    "    figw = 9\n",
    "    fig = plt.figure(figsize=(2*figw, figw))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    #fig = plt.figure(2)\n",
    "    #ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim([0, imw])\n",
    "    ax.set_ylim([0, imh])\n",
    "    plt.gca().invert_yaxis()\n",
    "    #ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    \n",
    "    # print(df.columns)\n",
    "    flt = (df.ridge_intensity>=df.predict_ci_low)& (df.ridge_intensity<=df.predict_ci_upp)\n",
    "    nonzero = df.slope != 0\n",
    "    #ax.plot(df.loc[flt ,'offsetx'], 1024-1-df.loc[flt, 'offsety'], 'b.', markersize=3, label='ridge_intensity In prediction range')\n",
    "    ax.plot(df.loc[flt ,'offsetx'], df.loc[flt, 'offsety'], 'b.', markersize=2, label='ridge_intensity In prediction range')\n",
    "    ax.plot(df.loc[(~flt)&nonzero ,'offsetx'], df.loc[(~flt)&nonzero, 'offsety'], 'co', markersize=5, label='Rg Out prediction range, slope!=0')\n",
    "    ax.plot(df.loc[(~flt)&(~nonzero) ,'offsetx'], df.loc[(~flt)&(~nonzero), 'offsety'], 'rd', markersize=5, label='Rg Out prediction range, slope==0')\n",
    "    ax.set_title(winname+\" Rg outside prediction range of $Rg={:.3f}slope+{:.3f}$\".format(k, b))\n",
    "    ax.legend()\n",
    "    \n",
    "    ## ridge_intensity > thresh filter plot\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim([0, imw])\n",
    "    ax.set_ylim([0, imh])\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    thresh = 0.003\n",
    "    flt = df.loc[:, 'ridge_intensity'] > thresh\n",
    "    ax.plot(df.loc[flt, 'offsetx'], df.loc[flt, 'offsety'], 'g.', markersize=2, label='ridge_intensity>{}'.format(thresh))\n",
    "    ax.plot(df.loc[~flt, 'offsetx'], df.loc[~flt, 'offsety'], 'r.', markersize=3, label='ridge_intensity<={}'.format(thresh))\n",
    "    \n",
    "    ax.set_title(winname+\" Rg>{} filter\".format(thresh))\n",
    "    ax.legend()\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #resid=y-y_pred\n",
    "    #rss=np.sum(resid**2)\n",
    "    #MSE=np.sqrt(rss/(result.nobs-2))\n",
    "    \n",
    "    def ols_quantile(m, X, q):\n",
    "      # m: Statsmodels OLS model.\n",
    "      # X: X matrix of data to predict.\n",
    "      # q: Quantile.\n",
    "      #\n",
    "      from scipy.stats import norm\n",
    "      mean_pred = m.predict(X)\n",
    "      se = np.sqrt(m.scale)\n",
    "      return mean_pred + norm.ppf(q) * se\n",
    "    \n",
    "    #print(ols_quantile(results, X, 0.5))\n",
    "    return results\n",
    "\n",
    "    \n",
    "results = plot_reg(ca)\n",
    "print(\"ridge_intensity v.s. slope regression results:\")\n",
    "print(results.summary())\n",
    "print('\\nresults.mse\\n', results.mse_resid, results.mse_total, '\\n')\n",
    "print(\"results.params\\n\", results.params, type(results.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patterns = [1, 444, 461, 1001, 3658]\n",
    "contourfiles= [CWD+'/{}_image_contour.txt'.format(pid) for pid in patterns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- for slightly better for visualazation\n",
    "\n",
    "    %matplotlib notebook \n",
    "\n",
    "- normal\n",
    "\n",
    "    %matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "### ridge_intensity v.s. slope regression plot for more patterns\n",
    "for ix, contourfile in enumerate(contourfiles):\n",
    "    ca = ContourAnalyzer(contourfile)\n",
    "    iminfo = 'Pattern '+str(patterns[ix])\n",
    "    plot_reg(ca, iminfo)\n",
    "    #plot_rd_filter(ca, iminfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "def plot_reg2(df):\n",
    "    colstr = 'slope  ridge_intensity'\n",
    "    cols = colstr.split()\n",
    "    df = df[cols]\n",
    "    df.loc[:, 'slope'] = df.loc[:, 'slope'].abs().values\n",
    "    x, y = df.loc[:, 'slope'], df.loc[:, 'ridge_intensity']\n",
    "\n",
    "    from scipy import stats\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x, y, 'o', label='original '+colstr)\n",
    "    ax.plot(x, intercept + slope*x, 'r', label='ridge_intensity={:.2f}slope+{:.2f}'.format(slope, intercept))\n",
    "    ax.set_xlim([0, x.max()*1.1])\n",
    "    ax.set_ylabel(r\"ridge_intensity\")\n",
    "    ax.set_xlabel(\"slope\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_reg2(ca.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NeighborParalism': 'NeighborParalism<0.98', 'NeighborOrientation': 'NeighborOrientation<0.98'}\n"
     ]
    }
   ],
   "source": [
    "def addNeighborFeatures(df):\n",
    "    '''\n",
    "    add Features for the input contour DataFrame, based on the neighbor relationship in the context of segment\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: [in, out] contour as DataFrame\n",
    "        [in] Contour df, must contains `polygonId`, `angle`, `offsetx`, `offsety`\n",
    "        [out] Contour df, added `NeighborContinuity`, `NeighborOrientation`, `NeighborParalism`\n",
    "\n",
    "            - `NeighborContinuity`:  |X(n) - X(n-1)|^2, usually is to 1 (because of 8-neighbor contour tracing)\n",
    "            - `NeighborOrientation`:  dot(EigenVector(n), EigenVector(n-1)), closer to 1, the better(may use 1-dot)\n",
    "            - `NeighborParalism`:  ||cross((X(n) - X(n-1)), EigenVector(n-1))||, closer to 1, the better(may use 1-cross)\n",
    "    TODO, the segment neighborhood based features can only be obtained by the whole segment, can't use ROI cropped segment \n",
    "    '''\n",
    "    if len(df) <= 0:\n",
    "        return df\n",
    "    polygonIds = df.loc[:, 'polygonId'].drop_duplicates().values\n",
    "    preIdx = df.index[0]\n",
    "    for polygonId in polygonIds:\n",
    "        isPolygonHead = True\n",
    "        for curIdx, _ in df.loc[df['polygonId']==polygonId, :].iterrows():\n",
    "            NeighborContinuity = 1\n",
    "            NeighborOrientation = 1\n",
    "            NeighborParalism = 1\n",
    "            if not isPolygonHead:\n",
    "                eigenvector_n_1 = np.array([np.cos(df.loc[preIdx, 'angle']), np.sin(df.loc[preIdx, 'angle'])])\n",
    "                eigenvector_n = np.array([np.cos(df.loc[curIdx, 'angle']), np.sin(df.loc[curIdx, 'angle'])])\n",
    "                neighorvector = np.array([df.loc[curIdx, 'offsetx'] - df.loc[preIdx, 'offsetx'],\n",
    "                                        df.loc[curIdx, 'offsety'] - df.loc[preIdx, 'offsety']])\n",
    "                crossvector = np.cross(neighorvector, eigenvector_n_1)\n",
    "\n",
    "                NeighborContinuity = np.sqrt(neighorvector.dot(neighorvector))\n",
    "                NeighborOrientation = eigenvector_n.dot(eigenvector_n_1)\n",
    "                NeighborParalism = np.sqrt(crossvector.dot(crossvector))/NeighborContinuity\n",
    "                NeighborContinuity = NeighborContinuity\n",
    "            preIdx = curIdx\n",
    "            isPolygonHead = False\n",
    "\n",
    "            for ii, val in enumerate([NeighborContinuity, NeighborOrientation, NeighborParalism]):\n",
    "                colname = allNeighborColNames[ii]\n",
    "                df.loc[curIdx, colname] = val\n",
    "    return df\n",
    "\n",
    "def plot_multi_filters(ca, patternid='', strFlts=None, transform_filter=False):\n",
    "    if strFlts is None:\n",
    "        newStrFlts = {}\n",
    "    elif not isinstance(strFlts, dict):\n",
    "        newStrFlts = {}\n",
    "        for col in allNeighborColNames:\n",
    "            for strFlt in strFlts:\n",
    "                if col in strFlt:\n",
    "                    newStrFlts[col] = strFlt\n",
    "                    break\n",
    "    strFlts = newStrFlts\n",
    "    print(strFlts)\n",
    "    df = ca.df\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    xini, yini, xend, yend = ca.contour.getBBox()\n",
    "    ax.set_xlim([xini, xend])\n",
    "    ax.set_ylim([yini, yend])\n",
    "    ax.set_title(\"Pattern \"+patternid)\n",
    "    \n",
    "    # plot contour\n",
    "    ax.plot(df.loc[:, 'offsetx'], df.loc[:, 'offsety'], 'k.', markersize=1, label='SEM Contour')\n",
    "    # plot angle\n",
    "    for _, row in df.iterrows():\n",
    "        x, y = row.loc['offsetx'], row.loc['offsety']\n",
    "        angle = row.loc['angle']\n",
    "        arrow_length = 1\n",
    "        dx, dy = arrow_length*np.cos(angle), arrow_length*np.sin(angle)\n",
    "        ax.arrow(x, y, dx, dy, width=0.1, fc='y', ec='y') # ,shape='right', overhang=0\n",
    "\n",
    "    # plot filters\n",
    "    if not transform_filter:\n",
    "        for strFlt in strFlts.values():\n",
    "            curdf = df.query(strFlt)\n",
    "            ax.plot(curdf.loc[:, 'offsetx'], curdf.loc[:, 'offsety'], 'o', markersize=4, label=strFlt, alpha=0.6)\n",
    "    else:\n",
    "        '''\n",
    "        inflection_points = []\n",
    "        for strFlt in strFlts.values():\n",
    "            curdf = df.query(strFlt )\n",
    "            print(strFlt, len(curdf))\n",
    "            if len(curdf) != 0:\n",
    "                inflection_points.append(curdf)\n",
    "        inflection_df = pd.concat(inflection_points)\n",
    "        '''\n",
    "        inflection_df = df.query('or '.join(strFlts.values()))\n",
    "        print(inflection_df[['polygonId', 'offsetx', 'offsety'] + allNeighborColNames[1:]])\n",
    "        \n",
    "        polygonIds = inflection_df.loc[:, 'polygonId'].drop_duplicates().values\n",
    "        for polygonId in polygonIds:\n",
    "            curdf = inflection_df.loc[inflection_df['polygonId']==polygonId, :]\n",
    "            maxNeighborContinuity = curdf.max()['NeighborContinuity']\n",
    "            minNeighborContinuity, minNeighborOrientation, minNeighborParalism = curdf.min()[allNeighborColNames]\n",
    "\n",
    "            if minNeighborParalism < minNeighborOrientation and len(curdf.query(strFlts['NeighborParalism'])) > 0:\n",
    "                idxmin = curdf['NeighborParalism'].idxmin()\n",
    "                ax.plot(curdf.loc[idxmin, 'offsetx'], curdf.loc[idxmin, 'offsety'], 'ro', markersize=4, label='minNeighborParalism', alpha=0.6)\n",
    "            elif minNeighborOrientation < minNeighborParalism and len(curdf.query(strFlts['NeighborOrientation'])) > 0:\n",
    "                idxmin = curdf['NeighborOrientation'].idxmin()\n",
    "                ax.plot(curdf.loc[idxmin, 'offsetx'], curdf.loc[idxmin, 'offsety'], 'rd', markersize=4, label='minNeighborOrientation', alpha=0.6)\n",
    "            elif len(curdf.query(strFlts['NeighborContinuity'])) > 0:\n",
    "                if abs(maxNeighborContinuity-1) > abs(minNeighborContinuity-1):\n",
    "                    idxmax = curdf['NeighborContinuity'].idxmax()\n",
    "                    ax.plot(curdf.loc[idxmax, 'offsetx'], curdf.loc[idxmax, 'offsety'], 'bd', markersize=4, label='maxNeighborContinuity', alpha=0.6)\n",
    "                else:\n",
    "                    idxmin = curdf['NeighborContinuity'].idxmin()\n",
    "                    ax.plot(curdf.loc[idxmin, 'offsetx'], curdf.loc[idxmin, 'offsety'], 'bo', markersize=4, label='minNeighborContinuity', alpha=0.6)\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #inflection_df.plot.scatter(x=allNeighborColNames[1], y=allNeighborColNames[2])\n",
    "    #plt.show()\n",
    "\n",
    "df = addNeighborFeatures(df)\n",
    "ca.df = df\n",
    "plot_multi_filters(ca, patternid=patternid, strFlts=['NeighborParalism<0.98', 'NeighborOrientation<0.98'], transform_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10650698, 0.78698604, 0.10650698]\n"
     ]
    }
   ],
   "source": [
    "def calcMeanOfLargestHistBin(arr, bins=10):\n",
    "    hist, bin_edges = np.histogram(arr, bins=bins)\n",
    "    idxmax = np.argmax(hist)\n",
    "    binvals = arr[np.where(np.logical_and(arr>=bin_edges[idxmax], arr<bin_edges[idxmax+1]))]\n",
    "    return np.mean(binvals)\n",
    "\n",
    "def findIndexOfFirstFlat(arr, gradients=None, start_pos=0, thres=None):\n",
    "    if gradients is None:\n",
    "        gradients = np.gradient(arr, edge_order=2)\n",
    "    assert(len(arr) == len(gradients))\n",
    "    absGradients = np.abs(gradients)\n",
    "    if thres is None:\n",
    "        thres = calcMeanOfLargestHistBin(absGradients)\n",
    "    for ix in range(start_pos, len(gradients)):\n",
    "        if absGradients[ix] < thres:\n",
    "            print(thres, ix)\n",
    "            return ix\n",
    "    return start_pos\n",
    "\n",
    "def findIndexOfFirstZeroCrossing(arr, gradients=None, start_pos=0):\n",
    "    if gradients is None:\n",
    "        gradients = np.gradient(arr, edge_order=2)\n",
    "    assert(len(arr) == len(gradients))\n",
    "    start = start_pos+1 if start_pos == 0 else start_pos\n",
    "    for ix in range(start_pos, len(gradients)):\n",
    "        if gradients[ix]*gradients[ix-1] <=0 :\n",
    "            return ix\n",
    "    return start_pos\n",
    "\n",
    "\n",
    "flt = [0.10650698, 0.78698604, 0.10650698] # sigma = 0.5\n",
    "#flt = [0.17, 0.66, 0.17] # sigma 0.6\n",
    "#flt = [0.25, 0.5, 0.25] # sigma 0.8\n",
    "print(flt)\n",
    "def smoothSignal(arr):\n",
    "    padArr = np.zeros((arr.shape[0]+2,) )\n",
    "    padArr[0] = arr[0]\n",
    "    padArr[1:-1] = arr\n",
    "    padArr[-1] = arr[-1]\n",
    "    '''\n",
    "    arr = list(arr)\n",
    "    padArr = arr[0:1] + arr[:] + arr[-1:]\n",
    "    '''\n",
    "    newArr = np.convolve(padArr, flt, 'valid')\n",
    "    if len(arr) != len(newArr):\n",
    "        raise ValueError(\"inequal length {} {}\".format(len(arr), len(newArr)))\n",
    "    return newArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7.6928015933137994e-05, 10)\n",
      "(7.3340165057348593e-05, 13)\n",
      "(14, [['NeighborOrientation', 6, 10], ['NeighborParalism', 1, 13]])\n",
      "(0.00013276157630110971, 7)\n",
      "(16, [['NeighborParalism', 1, 7], None])\n",
      "(27, [None, None])\n",
      "(28, [None, None])\n",
      "(0.00021663086250959125, 7)\n",
      "(29, [None, ['NeighborParalism', 2, 7]])\n",
      "(0.00066472856407347853, 5)\n",
      "(31, [['NeighborParalism', 1, 5], None])\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "def categorizeFilters(filters):\n",
    "    if filters is None:\n",
    "        newfilters = {}\n",
    "    elif not isinstance(filters, dict):\n",
    "        newfilters = {}\n",
    "        for col in allNeighborColNames:\n",
    "            for strFlt in filters:\n",
    "                if col in strFlt:\n",
    "                    newfilters[col] = strFlt\n",
    "                    break\n",
    "    filters = newfilters\n",
    "    # print(filters)\n",
    "    return filters\n",
    "\n",
    "def applyNeighborRuleModelPerVLine(linedf, filters, maxTailLenth=20, smooth=True):\n",
    "    dominant_issues = []\n",
    "    linedf.loc[:, 'ClfLabel'] = 1\n",
    "\n",
    "    # step 1, search and apply from head\n",
    "    headdf = linedf.loc[linedf.index[:maxTailLenth], :]\n",
    "    minNeighborOrientation, minNeighborParalism = headdf.min()[allNeighborColNames[1:]]\n",
    "    issue_feature, issue_index = None, None\n",
    "    if minNeighborParalism < minNeighborOrientation and len(headdf.query(filters['NeighborParalism'])) > 0:\n",
    "        issue_feature = 'NeighborParalism'\n",
    "        issue_index = np.argmin(headdf[issue_feature].values)\n",
    "    elif minNeighborOrientation < minNeighborParalism and len(headdf.query(filters['NeighborOrientation'])) > 0:\n",
    "        issue_feature = 'NeighborOrientation'\n",
    "        issue_index = np.argmin(headdf[issue_feature].values)\n",
    "    dominant_issues.append(None)\n",
    "    if issue_feature is not None:\n",
    "        dominant_issues[0] = [issue_feature, issue_index]\n",
    "        arr = linedf[issue_feature].values\n",
    "        if smooth:\n",
    "            arr = smoothSignal(arr)\n",
    "        gradient = np.gradient(arr, edge_order=2)\n",
    "        idxFlat = findIndexOfFirstFlat(arr, gradient, start_pos=issue_index+1)\n",
    "        dominant_issues[0].append(idxFlat)\n",
    "        idxFlat = min(maxTailLenth, idxFlat)\n",
    "        linedf.loc[linedf.index[:idxFlat], 'ClfLabel'] = 0\n",
    "\n",
    "    # step 2, search and apply from tail, reverse order\n",
    "    dominant_issues.append(None)\n",
    "    head_index = 0 if issue_index is None else issue_index\n",
    "    tailrange = len(linedf) - (head_index + 1) # exclude the head issue index itself\n",
    "    if tailrange > maxTailLenth:\n",
    "        taildf = linedf.loc[linedf.index[-maxTailLenth:], :]\n",
    "        minNeighborOrientation, minNeighborParalism = taildf.min()[allNeighborColNames[1:]]\n",
    "        issue_feature, issue_index = None, None\n",
    "        if minNeighborParalism < minNeighborOrientation and len(taildf.query(filters['NeighborParalism'])) > 0:\n",
    "            issue_feature = 'NeighborParalism'\n",
    "            issue_index = np.argmin(taildf[issue_feature].values)\n",
    "            issue_index = maxTailLenth - 1 - issue_index  # use index start from tail\n",
    "        elif minNeighborOrientation < minNeighborParalism and len(taildf.query(filters['NeighborOrientation'])) > 0:\n",
    "            issue_feature = 'NeighborOrientation'\n",
    "            issue_index = np.argmin(taildf[issue_feature].values)\n",
    "            issue_index = maxTailLenth - 1 - issue_index\n",
    "        if issue_feature is not None and issue_index:\n",
    "            dominant_issues[1] = [issue_feature, issue_index]\n",
    "            arr = linedf[issue_feature].values[::-1]\n",
    "            if smooth:\n",
    "                arr = smoothSignal(arr)\n",
    "            gradient = np.gradient(arr, edge_order=2)\n",
    "            idxFlat = findIndexOfFirstFlat(arr, gradient, start_pos=issue_index+1)\n",
    "            dominant_issues[1].append(idxFlat)\n",
    "            idxFlat = min(maxTailLenth, idxFlat)\n",
    "            linedf.loc[linedf.index[-idxFlat:], 'ClfLabel'] = 0\n",
    "    return linedf, dominant_issues\n",
    "\n",
    "def applyNeighborRuleModel(contourdf, filters, smooth=True):\n",
    "    '''\n",
    "    The step to find rule model in python:\n",
    "    1. apply combined filters to find ill contour Vline candidates\n",
    "    2. find the Vline candidates have dominant issues in its head+20 or tail-20\n",
    "    3. remove contour issue head/tail by following rules(default is 3.1):\n",
    "        * 3.1: search start from dominant issue position, new head=Index[1st flat gradient point]\n",
    "        * 3.2: Index[dominant issue position], new head=Index[the gradient zero-crossing point]\n",
    "    '''\n",
    "    maxTailLenth = 20\n",
    "    contourdf.loc[:, 'ClfLabel'] = 1\n",
    "    filters = categorizeFilters(filters)\n",
    "\n",
    "    inflection_df = contourdf.query('or '.join(filters.values()))\n",
    "    # print(inflection_df[['polygonId', 'offsetx', 'offsety'] + allNeighborColNames[1:]])\n",
    "    polygonIds = inflection_df.loc[:, 'polygonId'].drop_duplicates().values\n",
    "    for polygonId in polygonIds:\n",
    "        lineFlt = contourdf['polygonId']==polygonId\n",
    "        linedf = contourdf.loc[lineFlt, :]\n",
    "        newlinedf, dominant_issues = applyNeighborRuleModelPerVLine(linedf, filters, maxTailLenth, smooth)\n",
    "        print(int(polygonId), dominant_issues)\n",
    "        contourdf.loc[lineFlt, :] = newlinedf\n",
    "    return contourdf\n",
    "\n",
    "df = applyNeighborRuleModel(df, ['NeighborOrientation<0.98', 'NeighborParalism<0.98'])\n",
    "ca.df = df\n",
    "#plot_col_filter(ca, patternid='461', colname='ClfLabel')\n",
    "plot_col_by_label(ca, patternid=patternid, colname='ClfLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
