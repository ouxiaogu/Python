{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os.path\n",
    "sys.path.insert(0, os.getcwd()+\"/..\")\n",
    "from SEMContour import *\n",
    "sys.path.insert(0, os.getcwd()+\"/../../common\")\n",
    "from FileUtil import gpfs2WinPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in contour (image) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "#CWD = '/gpfs/WW/BD/MXP/SEM_IMAGE/IMEC/Case02_calaveras_v3/3Tmp/CT_KPI_test/Calaveras_v3_regular_CT_KPI_003_slope_modified_revert_all_patterns/h/cache/dummydb/result/MXP/job1/ContourSelectModelCalibration430result1'\n",
    "CWD = r'C:\\Localdata\\D\\Note\\Python\\apps\\ContourSelect\\samplejob\\h\\cache\\dummydb\\result\\MXP\\job1\\ContourSelectModelCalibration430result1'\n",
    "#CWD = r'C:\\Localdata\\D\\Note\\Python\\apps\\MXP\\ContourSelect\\samplejob1\\h\\cache\\dummydb\\result\\MXP\\job1\\ContourExtraction400result1'\n",
    "CWD = gpfs2WinPath(CWD)\n",
    "allNeighborColNames = ['NeighborContinuity', 'NeighborOrientation', 'NeighborParalism']\n",
    "\n",
    "class ContourAnalyzer(object):\n",
    "    \"\"\"docstring for ContourData\"\"\"\n",
    "    def __init__(self, contourfile):\n",
    "        self.__build(contourfile)\n",
    "\n",
    "    def __build(self, contourfile):\n",
    "        contour = SEMContour()\n",
    "        contour.parseFile(contourfile)\n",
    "        if not contour:\n",
    "            sys.exit(\"ERROR: read in contour file %s fails\\n\" % contourfile)\n",
    "        self.contour = contour\n",
    "        self.df = contour.toDf()\n",
    "# get contour data\n",
    "patternid = '461'\n",
    "contourfile = os.path.join(CWD, patternid+'_image_contour.txt')\n",
    "ca = ContourAnalyzer(contourfile)\n",
    "df = ca.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# contour plot utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_corr(df):\n",
    "    matplotlib.style.use('ggplot')\n",
    "    #plot_contour(self.contour)\n",
    "    # cols = 'slope  ridge_intensity intensity  contrast'.split()\n",
    "    cols = 'slope  ridge_intensity'.split()\n",
    "    print(df.columns)\n",
    "    df = df[cols]\n",
    "    df.loc[:, 'slope'] = df.loc[:, 'slope'].abs().values\n",
    "    \n",
    "    from pandas.plotting import scatter_matrix\n",
    "    colors = ['red','blue']\n",
    "    scatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal='kde', color=colors) \n",
    "    \n",
    "    '''\n",
    "    import seaborn as sns\n",
    "    sns.set(style=\"ticks\")\n",
    "    sns.pairplot(df, kind='scatter', diag_kind='kde')\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the SEM contour and angle\n",
    "def plot_contour_angle(ca, patternid='', arrow_length=1):\n",
    "    df = ca.df\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    imw, imh = ca.contour.getshape()\n",
    "    ax.set_xlim([0, imw])\n",
    "    ax.set_ylim([0, imh])\n",
    "    ax.set_title(\"Pattern \"+patternid+ \" image Contour\")\n",
    "    \n",
    "    # plot image\n",
    "    \n",
    "    # plot contour\n",
    "    ax.plot(df.loc[:, 'offsetx'], df.loc[:, 'offsety'], 'b.')\n",
    "    \n",
    "    # plot angle\n",
    "    for _, row in df.iterrows():\n",
    "        x, y = row.loc['offsetx'], row.loc['offsety']\n",
    "        angle = row.loc['angle']\n",
    "        dx, dy = arrow_length*np.cos(angle), arrow_length*np.sin(angle)\n",
    "        ax.arrow(x, y, dx, dy, width=0.1, fc='y', ec='y') # ,shape='right', overhang=0\n",
    "        \n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "plot_contour_angle(ca, patternid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the histgram for the modified slope, & plot by filter\n",
    "print(df.columns)\n",
    "colname = 'slope'\n",
    "df[colname].plot.hist(bins=100)\n",
    "def plot_col_filter(ca, patternid='', colname=''):\n",
    "    df = ca.df\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    xini, yini, xend, yend = ca.contour.getBBox()\n",
    "    ax.set_xlim([xini, xend])\n",
    "    ax.set_ylim([yini, yend])\n",
    "    ax.set_title(\"Pattern \"+patternid)\n",
    "    \n",
    "    thresh = 0\n",
    "    flt_gt = df.loc[:, colname] > thresh\n",
    "    flt_eq = df.loc[:, colname] == thresh\n",
    "    flt_lt = df.loc[:, colname] < thresh\n",
    "    \n",
    "    ax.plot(df.loc[flt_gt, 'offsetx'], df.loc[flt_gt, 'offsety'], 'b.', markersize=2, label=colname+'>{}'.format(thresh))\n",
    "    ax.plot(df.loc[flt_eq, 'offsetx'], df.loc[flt_eq, 'offsety'], 'ro', markersize=2, label=colname+'=={}'.format(thresh))\n",
    "    ax.plot(df.loc[flt_lt, 'offsetx'], df.loc[flt_lt, 'offsety'], 'r.', markersize=2, label=colname+'<{}'.format(thresh))\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_col_filter(ca, patternid=patternid, colname=colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot by column unique labels\n",
    "def plot_col_by_label(contour, patternid='', colname='', extend=False):\n",
    "    df = contour.toDf()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    xini, yini, xend, yend = contour.getBBox()\n",
    "    ax.set_xlim([xini, xend])\n",
    "    ax.set_ylim([yini, yend])\n",
    "    ax.set_title(\"Pattern \"+patternid)\n",
    "    \n",
    "    uniqLabels = df.loc[:, colname].drop_duplicates().values\n",
    "    print(df.loc[:, colname].value_counts())\n",
    "    for label in uniqLabels:\n",
    "        flt_eq = df.loc[:, colname] == label\n",
    "        if label == 'nan':\n",
    "            flt_eq = df.loc[:, colname].isna()\n",
    "        ax.plot(df.loc[flt_eq, 'offsetx'], df.loc[flt_eq, 'offsety'], '.', linestyle='None',  markersize=2, label=colname+'=={}'.format(label))\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    if extend:\n",
    "        return plt, ax\n",
    "    else:\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour Clf Label plot for 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/gpfs/WW/BD/MXP/SHARED/SEM_IMAGE/IMEC/Case02_calaveras_v3/3Tmp/ContourSelection/007_ContourSel_Tree_debug/h/cache/dummydb/result/MXP/job1/ContourSelection450result1/461_contour_selected.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a5e6e568e276>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcontourfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/gpfs/WW/BD/MXP/SHARED/SEM_IMAGE/IMEC/Case02_calaveras_v3/3Tmp/ContourSelection/007_ContourSel_Tree_debug/h/cache/dummydb/result/MXP/job1/ContourSelection450result1/461_contour_selected.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcontour\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSEMContour\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcontour\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparseFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontourfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcontourdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontour\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Localdata\\D\\Note\\Python\\libs\\tacx\\unittest/..\\SEMContour.pyc\u001b[0m in \u001b[0;36mparseFile\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparseFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileHandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;31m# Read first line: field size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\s+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/gpfs/WW/BD/MXP/SHARED/SEM_IMAGE/IMEC/Case02_calaveras_v3/3Tmp/ContourSelection/007_ContourSel_Tree_debug/h/cache/dummydb/result/MXP/job1/ContourSelection450result1/461_contour_selected.txt'"
     ]
    }
   ],
   "source": [
    "#contourfile = r'/gpfs/WW/BD/MXP/SHARED/SEM_IMAGE/IMEC/Case02_calaveras_v3/3Tmp/CT_KPI_test/Calaveras_v3_regular_CT_KPI_003_slope_modified_revert_all_patterns/h/cache/dummydb/result/MXP/job1/ContourSelectModelCalibration452result1/461_image_contour.txt'\n",
    "contourfile='/gpfs/WW/BD/MXP/SHARED/SEM_IMAGE/IMEC/Case02_calaveras_v3/3Tmp/ContourSelection/007_ContourSel_Tree_debug/h/cache/dummydb/result/MXP/job1/ContourSelection450result1/461_contour_selected.txt'\n",
    "contour = SEMContour()\n",
    "contour.parseFile(contourfile)\n",
    "contourdf = contour.toDf()\n",
    "\n",
    "df = contourdf\n",
    "\n",
    "plot_col_by_label(contour, '461', 'ClfLabel', False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour Label : connect discrete sub-segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    8043\n",
      "0.0     144\n",
      "Name: ClfLabel, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from SEMContourUtil import connectDiscreteSubSegment\n",
    "#contourfile = r'../439_image_contour.txt'\n",
    "#contourfile = r'../439_image_contour_new.txt'\n",
    "contourfile = r'../461_contour_selected_new.txt'\n",
    "contour = SEMContour()\n",
    "contour.parseFile(contourfile)\n",
    "contourdf = contour.toDf()\n",
    "#df = connectDiscreteSubSegment(contourdf)\n",
    "#dst = contour.fromDf(df)\n",
    "df = contourdf\n",
    "#plot_col_by_label(contour, '439', 'ClfLabel', False)\n",
    "\n",
    "plt, ax = plot_col_by_label(contour, '461', 'ClfLabel', True)\n",
    "asKept = (df.loc[:, 'ClfLabel'] == 0) & (df.loc[:, 'NewLabel'] == 1)\n",
    "ax.plot(df.loc[asKept, 'offsetx'], df.loc[asKept, 'offsety'], 'o', linestyle='None',  \n",
    "        markersize=6, markeredgewidth=1, markerfacecolor='none', label='removed to Kept', alpha=0.6)\n",
    "asRemoved = (df.loc[:, 'ClfLabel'] == 1) & (df.loc[:, 'NewLabel'] == 0)\n",
    "ax.plot(df.loc[asRemoved, 'offsetx'], df.loc[asRemoved, 'offsety'], 'd', linestyle='None',  \n",
    "        markersize=6, markeredgewidth=1, markerfacecolor='none', label='Kept to Removed', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour Selection Rule model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addNeighborFeatures(df):\n",
    "    '''\n",
    "    add Features for the input contour DataFrame, based on the neighbor relationship in the context of segment\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: [in, out] contour as DataFrame\n",
    "        [in] Contour df, must contains `polygonId`, `angle`, `offsetx`, `offsety`\n",
    "        [out] Contour df, added `NeighborContinuity`, `NeighborOrientation`, `NeighborParalism`\n",
    "\n",
    "            - `NeighborContinuity`:  |X(n) - X(n-1)|^2, usually is to 1 (because of 8-neighbor contour tracing)\n",
    "            - `NeighborOrientation`:  dot(EigenVector(n), EigenVector(n-1)), closer to 1, the better(may use 1-dot)\n",
    "            - `NeighborParalism`:  ||cross((X(n) - X(n-1)), EigenVector(n-1))||, closer to 1, the better(may use 1-cross)\n",
    "    TODO, the segment neighborhood based features can only be obtained by the whole segment, can't use ROI cropped segment \n",
    "    '''\n",
    "    if len(df) <= 0:\n",
    "        return df\n",
    "    polygonIds = df.loc[:, 'polygonId'].drop_duplicates().values\n",
    "    preIdx = df.index[0]\n",
    "    for polygonId in polygonIds:\n",
    "        isPolygonHead = True\n",
    "        for curIdx, _ in df.loc[df['polygonId']==polygonId, :].iterrows():\n",
    "            NeighborContinuity = 1\n",
    "            NeighborOrientation = 1\n",
    "            NeighborParalism = 1\n",
    "            if not isPolygonHead:\n",
    "                eigenvector_n_1 = np.array([np.cos(df.loc[preIdx, 'angle']), np.sin(df.loc[preIdx, 'angle'])])\n",
    "                eigenvector_n = np.array([np.cos(df.loc[curIdx, 'angle']), np.sin(df.loc[curIdx, 'angle'])])\n",
    "                neighorvector = np.array([df.loc[curIdx, 'offsetx'] - df.loc[preIdx, 'offsetx'],\n",
    "                                        df.loc[curIdx, 'offsety'] - df.loc[preIdx, 'offsety']])\n",
    "                crossvector = np.cross(neighorvector, eigenvector_n_1)\n",
    "\n",
    "                NeighborContinuity = np.sqrt(neighorvector.dot(neighorvector))\n",
    "                NeighborOrientation = eigenvector_n.dot(eigenvector_n_1)\n",
    "                NeighborParalism = np.sqrt(crossvector.dot(crossvector))/NeighborContinuity\n",
    "                NeighborContinuity = NeighborContinuity\n",
    "            preIdx = curIdx\n",
    "            isPolygonHead = False\n",
    "\n",
    "            for ii, val in enumerate([NeighborContinuity, NeighborOrientation, NeighborParalism]):\n",
    "                colname = allNeighborColNames[ii]\n",
    "                df.loc[curIdx, colname] = val\n",
    "    return df\n",
    "\n",
    "def plot_multi_filters(ca, patternid='', strFlts=None, transform_filter=False):\n",
    "    if strFlts is None:\n",
    "        newStrFlts = {}\n",
    "    elif not isinstance(strFlts, dict):\n",
    "        newStrFlts = {}\n",
    "        for col in allNeighborColNames:\n",
    "            for strFlt in strFlts:\n",
    "                if col in strFlt:\n",
    "                    newStrFlts[col] = strFlt\n",
    "                    break\n",
    "    strFlts = newStrFlts\n",
    "    print(strFlts)\n",
    "    df = ca.df\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    xini, yini, xend, yend = ca.contour.getBBox()\n",
    "    ax.set_xlim([xini, xend])\n",
    "    ax.set_ylim([yini, yend])\n",
    "    ax.set_title(\"Pattern \"+patternid)\n",
    "    \n",
    "    # plot contour\n",
    "    ax.plot(df.loc[:, 'offsetx'], df.loc[:, 'offsety'], 'k.', markersize=1, label='SEM Contour')\n",
    "    # plot angle\n",
    "    for _, row in df.iterrows():\n",
    "        x, y = row.loc['offsetx'], row.loc['offsety']\n",
    "        angle = row.loc['angle']\n",
    "        arrow_length = 1\n",
    "        dx, dy = arrow_length*np.cos(angle), arrow_length*np.sin(angle)\n",
    "        ax.arrow(x, y, dx, dy, width=0.1, fc='y', ec='y') # ,shape='right', overhang=0\n",
    "\n",
    "    # plot filters\n",
    "    if not transform_filter:\n",
    "        for strFlt in strFlts.values():\n",
    "            curdf = df.query(strFlt)\n",
    "            ax.plot(curdf.loc[:, 'offsetx'], curdf.loc[:, 'offsety'], 'o', markersize=4, label=strFlt, alpha=0.6)\n",
    "    else:\n",
    "        '''\n",
    "        inflection_points = []\n",
    "        for strFlt in strFlts.values():\n",
    "            curdf = df.query(strFlt )\n",
    "            print(strFlt, len(curdf))\n",
    "            if len(curdf) != 0:\n",
    "                inflection_points.append(curdf)\n",
    "        inflection_df = pd.concat(inflection_points)\n",
    "        '''\n",
    "        inflection_df = df.query('or '.join(strFlts.values()))\n",
    "        print(inflection_df[['polygonId', 'offsetx', 'offsety'] + allNeighborColNames[1:]])\n",
    "        \n",
    "        polygonIds = inflection_df.loc[:, 'polygonId'].drop_duplicates().values\n",
    "        for polygonId in polygonIds:\n",
    "            curdf = inflection_df.loc[inflection_df['polygonId']==polygonId, :]\n",
    "            maxNeighborContinuity = curdf.max()['NeighborContinuity']\n",
    "            minNeighborContinuity, minNeighborOrientation, minNeighborParalism = curdf.min()[allNeighborColNames]\n",
    "\n",
    "            if minNeighborParalism < minNeighborOrientation and len(curdf.query(strFlts['NeighborParalism'])) > 0:\n",
    "                idxmin = curdf['NeighborParalism'].idxmin()\n",
    "                ax.plot(curdf.loc[idxmin, 'offsetx'], curdf.loc[idxmin, 'offsety'], 'ro', markersize=4, label='minNeighborParalism', alpha=0.6)\n",
    "            elif minNeighborOrientation < minNeighborParalism and len(curdf.query(strFlts['NeighborOrientation'])) > 0:\n",
    "                idxmin = curdf['NeighborOrientation'].idxmin()\n",
    "                ax.plot(curdf.loc[idxmin, 'offsetx'], curdf.loc[idxmin, 'offsety'], 'rd', markersize=4, label='minNeighborOrientation', alpha=0.6)\n",
    "            elif len(curdf.query(strFlts['NeighborContinuity'])) > 0:\n",
    "                if abs(maxNeighborContinuity-1) > abs(minNeighborContinuity-1):\n",
    "                    idxmax = curdf['NeighborContinuity'].idxmax()\n",
    "                    ax.plot(curdf.loc[idxmax, 'offsetx'], curdf.loc[idxmax, 'offsety'], 'bd', markersize=4, label='maxNeighborContinuity', alpha=0.6)\n",
    "                else:\n",
    "                    idxmin = curdf['NeighborContinuity'].idxmin()\n",
    "                    ax.plot(curdf.loc[idxmin, 'offsetx'], curdf.loc[idxmin, 'offsety'], 'bo', markersize=4, label='minNeighborContinuity', alpha=0.6)\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    inflection_df.plot.scatter(x=allNeighborColNames[1], y=allNeighborColNames[2])\n",
    "    plt.show()\n",
    "\n",
    "df = addNeighborFeatures(df)\n",
    "ca.df = df\n",
    "plot_multi_filters(ca, patternid=patternid, strFlts=['abs(1-NeighborContinuity) > 0.5', 'NeighborParalism<0.98', 'NeighborOrientation<0.98'], transform_filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMeanOfLargestHistBin(arr, bins=10):\n",
    "    hist, bin_edges = np.histogram(arr, bins=bins)\n",
    "    idxmax = np.argmax(hist)\n",
    "    binvals = arr[np.where(np.logical_and(arr>=bin_edges[idxmax], arr<bin_edges[idxmax+1]))]\n",
    "    return np.mean(binvals)\n",
    "\n",
    "def findIndexOfFirstFlat(arr, gradients=None, start_pos=0, thres=None):\n",
    "    if gradients is None:\n",
    "        gradients = np.gradient(arr, edge_order=2)\n",
    "    assert(len(arr) == len(gradients))\n",
    "    absGradients = np.abs(gradients)\n",
    "    if thres is None:\n",
    "        thres = calcMeanOfLargestHistBin(absGradients)\n",
    "    for ix in range(start_pos, len(gradients)):\n",
    "        if absGradients[ix] < thres:\n",
    "            print(thres, ix)\n",
    "            return ix\n",
    "    return start_pos\n",
    "\n",
    "def findIndexOfFirstZeroCrossing(arr, gradients=None, start_pos=0):\n",
    "    if gradients is None:\n",
    "        gradients = np.gradient(arr, edge_order=2)\n",
    "    assert(len(arr) == len(gradients))\n",
    "    start = start_pos+1 if start_pos == 0 else start_pos\n",
    "    for ix in range(start_pos, len(gradients)):\n",
    "        if gradients[ix]*gradients[ix-1] <=0 :\n",
    "            return ix\n",
    "    return start_pos\n",
    "\n",
    "\n",
    "flt = [0.10650698, 0.78698604, 0.10650698] # sigma = 0.5\n",
    "flt = [0.17, 0.66, 0.17] # sigma 0.6\n",
    "flt = [0.25, 0.5, 0.25] # sigma 0.8\n",
    "print(flt)\n",
    "def smoothSignal(arr):\n",
    "    padArr = np.zeros((arr.shape[0]+2,) )\n",
    "    padArr[0] = arr[0]\n",
    "    padArr[1:-1] = arr\n",
    "    padArr[-1] = arr[-1]\n",
    "    '''\n",
    "    arr = list(arr)\n",
    "    padArr = arr[0:1] + arr[:] + arr[-1:]\n",
    "    '''\n",
    "    newArr = np.convolve(padArr, flt, 'valid')\n",
    "    if len(arr) != len(newArr):\n",
    "        raise ValueError(\"inequal length {} {}\".format(len(arr), len(newArr)))\n",
    "    return newArr\n",
    "\n",
    "for polygonId in [21, 8, 37,  38]: # 27,\n",
    "    print(polygonId)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    curdf = df.loc[df.polygonId==polygonId, :]\n",
    "    x = np.arange(len(curdf))\n",
    "    arr1 = curdf[allNeighborColNames[1]].values\n",
    "    arr2 = curdf[allNeighborColNames[2]].values\n",
    "    #arr2 = smoothSignal(arr2)\n",
    "    gradient1 = np.gradient(arr1, edge_order=2)\n",
    "    gradient2 = np.gradient(arr2, edge_order=2)\n",
    "    Paralism_xx = np.gradient(gradient2, edge_order=2)\n",
    "    \n",
    "    idxmin = np.argmin(arr2)\n",
    "    ax.plot(idxmin, arr2[idxmin], 'o', markersize=12, markeredgewidth=2, markerfacecolor='none', label= 'min'+allNeighborColNames[2])\n",
    "    idx1 = findIndexOfFirstFlat(arr2, gradient2, start_pos=idxmin+1)\n",
    "    idx2 = findIndexOfFirstZeroCrossing(arr2, gradient2, start_pos=idxmin+1)\n",
    "    ax.plot(idx1, arr2[idx1], 'o', markersize=12, markeredgewidth=2, markerfacecolor='none', label= allNeighborColNames[2] + ' zero gradient')\n",
    "    ax.plot(idx2, arr2[idx2], 'o', markersize=12, markeredgewidth=2, markerfacecolor='none', label= allNeighborColNames[2] + ' zero-crossing gradient')\n",
    "    \n",
    "    \n",
    "    #ax.plot(x, (curdf[allNeighborColNames[0]]-1).abs(), 'g-d', label=allNeighborColNames[0])\n",
    "    #ax.plot(x, curdf[allNeighborColNames[1]], '-o', label= allNeighborColNames[1])\n",
    "    ax.plot(x, curdf[allNeighborColNames[2]], '--o', label= allNeighborColNames[2], alpha=0.6)\n",
    "    ax.plot(x, arr2, '--o', label= allNeighborColNames[2], alpha=0.6)\n",
    "    #ax.plot(x, gradient1, '-*', label= allNeighborColNames[1] + ' gradient')\n",
    "    #ax.plot(x, gradient2, '--*', label= allNeighborColNames[2] + ' gradient')\n",
    "    #ax.plot(x, Paralism_xx, '--d', label= allNeighborColNames[2] + ' 2nd deriative')\n",
    "    \n",
    "    if idxmin in range(40):\n",
    "        ax.set_xlim([0, 40])\n",
    "    #plt.yscale('log')    \n",
    "    ax.set_title(\"Pattern {} Vline {}\".format(patternid, polygonId))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorizeFilters(filters):\n",
    "    if filters is None:\n",
    "        newfilters = {}\n",
    "    elif not isinstance(filters, dict):\n",
    "        newfilters = {}\n",
    "        for col in allNeighborColNames:\n",
    "            for strFlt in filters:\n",
    "                if col in strFlt:\n",
    "                    newfilters[col] = strFlt\n",
    "                    break\n",
    "    filters = newfilters\n",
    "    # print(filters)\n",
    "    return filters\n",
    "\n",
    "def applyNeighborRuleModelPerVLine(linedf, filters, maxTailLenth=20, smooth=True):\n",
    "    dominant_issues = []\n",
    "    linedf.loc[:, 'ClfLabel'] = 1\n",
    "\n",
    "    # step 1, search and apply from head\n",
    "    headdf = linedf.loc[linedf.index[:maxTailLenth], :]\n",
    "    minNeighborOrientation, minNeighborParalism = headdf.min()[allNeighborColNames[1:]]\n",
    "    issue_feature, issue_index = None, None\n",
    "    if minNeighborParalism < minNeighborOrientation and len(headdf.query(filters['NeighborParalism'])) > 0:\n",
    "        issue_feature = 'NeighborParalism'\n",
    "        issue_index = np.argmin(headdf[issue_feature].values)\n",
    "    elif minNeighborOrientation < minNeighborParalism and len(headdf.query(filters['NeighborOrientation'])) > 0:\n",
    "        issue_feature = 'NeighborOrientation'\n",
    "        issue_index = np.argmin(headdf[issue_feature].values)\n",
    "    dominant_issues.append(None)\n",
    "    if issue_feature is not None:\n",
    "        dominant_issues[0] = [issue_feature, issue_index]\n",
    "        arr = linedf[issue_feature].values\n",
    "        if smooth:\n",
    "            arr = smoothSignal(arr)\n",
    "        gradient = np.gradient(arr, edge_order=2)\n",
    "        idxFlat = findIndexOfFirstFlat(arr, gradient, start_pos=issue_index+1)\n",
    "        dominant_issues[0].append(idxFlat)\n",
    "        idxFlat = min(maxTailLenth, idxFlat)\n",
    "        linedf.loc[linedf.index[:idxFlat], 'ClfLabel'] = 0\n",
    "\n",
    "    # step 2, search and apply from tail, reverse order\n",
    "    dominant_issues.append(None)\n",
    "    head_index = 0 if issue_index is None else issue_index\n",
    "    tailrange = len(linedf) - (head_index + 1) # exclude the head issue index itself\n",
    "    if tailrange > maxTailLenth:\n",
    "        taildf = linedf.loc[linedf.index[-maxTailLenth:], :]\n",
    "        minNeighborOrientation, minNeighborParalism = taildf.min()[allNeighborColNames[1:]]\n",
    "        issue_feature, issue_index = None, None\n",
    "        if minNeighborParalism < minNeighborOrientation and len(taildf.query(filters['NeighborParalism'])) > 0:\n",
    "            issue_feature = 'NeighborParalism'\n",
    "            issue_index = np.argmin(taildf[issue_feature].values)\n",
    "            issue_index = maxTailLenth - 1 - issue_index  # use index start from tail\n",
    "        elif minNeighborOrientation < minNeighborParalism and len(taildf.query(filters['NeighborOrientation'])) > 0:\n",
    "            issue_feature = 'NeighborOrientation'\n",
    "            issue_index = np.argmin(taildf[issue_feature].values)\n",
    "            issue_index = maxTailLenth - 1 - issue_index\n",
    "        if issue_feature is not None and issue_index:\n",
    "            dominant_issues[1] = [issue_feature, issue_index]\n",
    "            arr = linedf[issue_feature].values[::-1]\n",
    "            if smooth:\n",
    "                arr = smoothSignal(arr)\n",
    "            gradient = np.gradient(arr, edge_order=2)\n",
    "            idxFlat = findIndexOfFirstFlat(arr, gradient, start_pos=issue_index+1)\n",
    "            dominant_issues[1].append(idxFlat)\n",
    "            idxFlat = min(maxTailLenth, idxFlat)\n",
    "            linedf.loc[linedf.index[-idxFlat:], 'ClfLabel'] = 0\n",
    "    return linedf, dominant_issues\n",
    "\n",
    "def applyNeighborRuleModel(contourdf, filters, smooth=True):\n",
    "    '''\n",
    "    The step to find rule model in python:\n",
    "    1. apply combined filters to find ill contour Vline candidates\n",
    "    2. find the Vline candidates have dominant issues in its head+20 or tail-20\n",
    "    3. remove contour issue head/tail by following rules(default is 3.1):\n",
    "        * 3.1: search start from dominant issue position, new head=Index[1st flat gradient point]\n",
    "        * 3.2: Index[dominant issue position], new head=Index[the gradient zero-crossing point]\n",
    "    '''\n",
    "    maxTailLenth = 20\n",
    "    contourdf.loc[:, 'ClfLabel'] = 1\n",
    "    filters = categorizeFilters(filters)\n",
    "\n",
    "    inflection_df = contourdf.query('or '.join(filters.values()))\n",
    "    # print(inflection_df[['polygonId', 'offsetx', 'offsety'] + allNeighborColNames[1:]])\n",
    "    polygonIds = inflection_df.loc[:, 'polygonId'].drop_duplicates().values\n",
    "    for polygonId in polygonIds:\n",
    "        lineFlt = contourdf['polygonId']==polygonId\n",
    "        linedf = contourdf.loc[lineFlt, :]\n",
    "        newlinedf, dominant_issues = applyNeighborRuleModelPerVLine(linedf, filters, maxTailLenth, smooth)\n",
    "        print(int(polygonId), dominant_issues)\n",
    "        contourdf.loc[lineFlt, :] = newlinedf\n",
    "    return contourdf\n",
    "\n",
    "df = applyNeighborRuleModel(df, ['NeighborOrientation<0.98', 'NeighborParalism<0.98'])\n",
    "ca.df = df\n",
    "#plot_col_filter(ca, patternid='461', colname='ClfLabel')\n",
    "plot_col_by_label(ca, patternid=patternid, colname='ClfLabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modified contour slope filter V.S. rigde_intensity filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot contour filtering by ridge_intensity \n",
    "def plot_rd_filter(ca, patternid=''):\n",
    "    df = ca.df\n",
    "    imw, imh = ca.contour.getshape()\n",
    "    \n",
    "    figw = 9\n",
    "    fig = plt.figure(figsize=(figw, figw))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim([0, imw])\n",
    "    ax.set_ylim([0, imh])\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    thresh = 0.003\n",
    "    flt = df.loc[:, 'ridge_intensity'] > thresh\n",
    "    ax.plot(df.loc[flt, 'offsetx'], df.loc[flt, 'offsety'], 'g.', markersize=2, label='ridge_intensity>{}'.format(thresh))\n",
    "    ax.plot(df.loc[~flt, 'offsetx'], df.loc[~flt, 'offsety'], 'r.', markersize=3, label='ridge_intensity<={}'.format(thresh))\n",
    "    \n",
    "    ax.set_title(patternid+\" Rg>{} filter\".format(thresh))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_rd_filter(ca, 'Pattern 3658')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for pop out plot\n",
    "%matplotlib qt5\n",
    "\n",
    "def plot_reg(ca, winname=''):\n",
    "    df = ca.df\n",
    "    imw, imh = ca.contour.getshape()\n",
    "    \n",
    "    colstr = 'slope  ridge_intensity'\n",
    "    cols = colstr.split()\n",
    "    #df = df[cols]\n",
    "    df.loc[:, 'slope'] = df.loc[:, 'slope'].abs().values\n",
    "    ## df = df.loc[df.slope<0.03, :]\n",
    "    x, y = df.loc[:, 'slope'], df.loc[:, 'ridge_intensity']\n",
    "\n",
    "    # from scipy import stats\n",
    "    # slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "    #plt.plot(x, intercept + slope*x, 'r', label='fitted ridge_intensity')\n",
    "    import statsmodels.api as sm\n",
    "    X = sm.add_constant(x, prepend=False)\n",
    "    results = sm.OLS(y, X).fit()\n",
    "    # print(results.summary())\n",
    "    # print(results.mse_resid, results.mse_total)\n",
    "    # print(results.params, type(results.params))\n",
    "    k, b = results.params.loc['slope'], results.params.loc['const']\n",
    "    \n",
    "    from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "    pred_std, predict_ci_low, predict_ci_upp = wls_prediction_std(results)\n",
    "\n",
    "    xmax, ymax = x.max(), y.max()\n",
    "    figw = 7\n",
    "    fig = plt.figure(figsize=(figw, figw*ymax/xmax))\n",
    "    #fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlim([0, xmax*1.1])\n",
    "    ax.set_ylim([0, ymax*1.1])\n",
    "    \n",
    "    ax.set_title(winname+' ridge_intensity v.s. abs of modified slope')\n",
    "    ax.set_xlabel('modified slope')\n",
    "    ax.set_ylabel('ridge_intensity')\n",
    "    \n",
    "    ax.plot(x, y, 'o', label='original ridge_intensity v.s. slope')\n",
    "    y_pred = results.predict()\n",
    "    ax.plot(x, y_pred, 'r', label='predicted ridge_intensity={:.3f}slope+{:.3f}, $R^2={:.3f}$'.format(k, b, results.rsquared))\n",
    "    plt.plot(x, predict_ci_low, 'b--', lw=1, label='predict lower')\n",
    "    plt.plot(x, predict_ci_upp, 'g--', lw=1, label='predict upper')\n",
    "    \n",
    "    df.loc[:, 'predict_ci_low'] = predict_ci_low\n",
    "    df.loc[:, 'predict_ci_upp'] = predict_ci_upp\n",
    "    \n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    ## ridge_intensity prediction boundary plot\n",
    "    figw = 9\n",
    "    fig = plt.figure(figsize=(2*figw, figw))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    #fig = plt.figure(2)\n",
    "    #ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim([0, imw])\n",
    "    ax.set_ylim([0, imh])\n",
    "    plt.gca().invert_yaxis()\n",
    "    #ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    \n",
    "    # print(df.columns)\n",
    "    flt = (df.ridge_intensity>=df.predict_ci_low)& (df.ridge_intensity<=df.predict_ci_upp)\n",
    "    nonzero = df.slope != 0\n",
    "    #ax.plot(df.loc[flt ,'offsetx'], 1024-1-df.loc[flt, 'offsety'], 'b.', markersize=3, label='ridge_intensity In prediction range')\n",
    "    ax.plot(df.loc[flt ,'offsetx'], df.loc[flt, 'offsety'], 'b.', markersize=2, label='ridge_intensity In prediction range')\n",
    "    ax.plot(df.loc[(~flt)&nonzero ,'offsetx'], df.loc[(~flt)&nonzero, 'offsety'], 'co', markersize=5, label='Rg Out prediction range, slope!=0')\n",
    "    ax.plot(df.loc[(~flt)&(~nonzero) ,'offsetx'], df.loc[(~flt)&(~nonzero), 'offsety'], 'rd', markersize=5, label='Rg Out prediction range, slope==0')\n",
    "    ax.set_title(winname+\" Rg outside prediction range of $Rg={:.3f}slope+{:.3f}$\".format(k, b))\n",
    "    ax.legend()\n",
    "    \n",
    "    ## ridge_intensity > thresh filter plot\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim([0, imw])\n",
    "    ax.set_ylim([0, imh])\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    thresh = 0.003\n",
    "    flt = df.loc[:, 'ridge_intensity'] > thresh\n",
    "    ax.plot(df.loc[flt, 'offsetx'], df.loc[flt, 'offsety'], 'g.', markersize=2, label='ridge_intensity>{}'.format(thresh))\n",
    "    ax.plot(df.loc[~flt, 'offsetx'], df.loc[~flt, 'offsety'], 'r.', markersize=3, label='ridge_intensity<={}'.format(thresh))\n",
    "    \n",
    "    ax.set_title(winname+\" Rg>{} filter\".format(thresh))\n",
    "    ax.legend()\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #resid=y-y_pred\n",
    "    #rss=np.sum(resid**2)\n",
    "    #MSE=np.sqrt(rss/(result.nobs-2))\n",
    "    \n",
    "    def ols_quantile(m, X, q):\n",
    "      # m: Statsmodels OLS model.\n",
    "      # X: X matrix of data to predict.\n",
    "      # q: Quantile.\n",
    "      #\n",
    "      from scipy.stats import norm\n",
    "      mean_pred = m.predict(X)\n",
    "      se = np.sqrt(m.scale)\n",
    "      return mean_pred + norm.ppf(q) * se\n",
    "    \n",
    "    #print(ols_quantile(results, X, 0.5))\n",
    "    return results\n",
    "\n",
    "    \n",
    "results = plot_reg(ca)\n",
    "print(\"ridge_intensity v.s. slope regression results:\")\n",
    "print(results.summary())\n",
    "print('\\nresults.mse\\n', results.mse_resid, results.mse_total, '\\n')\n",
    "print(\"results.params\\n\", results.params, type(results.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patterns = [1, 444, 461, 1001, 3658]\n",
    "contourfiles= [CWD+'/{}_image_contour.txt'.format(pid) for pid in patterns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- for slightly better for visualazation\n",
    "\n",
    "    %matplotlib notebook \n",
    "\n",
    "- normal\n",
    "\n",
    "    %matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "### ridge_intensity v.s. slope regression plot for more patterns\n",
    "for ix, contourfile in enumerate(contourfiles):\n",
    "    ca = ContourAnalyzer(contourfile)\n",
    "    iminfo = 'Pattern '+str(patterns[ix])\n",
    "    plot_reg(ca, iminfo)\n",
    "    #plot_rd_filter(ca, iminfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "def plot_reg2(df):\n",
    "    colstr = 'slope  ridge_intensity'\n",
    "    cols = colstr.split()\n",
    "    df = df[cols]\n",
    "    df.loc[:, 'slope'] = df.loc[:, 'slope'].abs().values\n",
    "    x, y = df.loc[:, 'slope'], df.loc[:, 'ridge_intensity']\n",
    "\n",
    "    from scipy import stats\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x, y, 'o', label='original '+colstr)\n",
    "    ax.plot(x, intercept + slope*x, 'r', label='ridge_intensity={:.2f}slope+{:.2f}'.format(slope, intercept))\n",
    "    ax.set_xlim([0, x.max()*1.1])\n",
    "    ax.set_ylabel(r\"ridge_intensity\")\n",
    "    ax.set_xlabel(\"slope\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_reg2(ca.df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
